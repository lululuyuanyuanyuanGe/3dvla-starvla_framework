from typing import Optional, Union, Dict, Any, List
from PIL import Image

class QwenVLPromptHelper_v1:
    def __init__( #@Jinhui è€ƒè™‘è¿™é‡Œæ˜¯å¦è¦å’Œæ¨¡å‹è§£å¶åˆ
        self,
        processor,
        system_prompt: str = "You are Qwen, created by Alibaba Cloud. You are a helpful assistant."
    ):
        """
        Helper for building and processing Qwen-VL style prompts.

        Args:
            processor: Qwen processor (AutoProcessor from Qwen-VL)
            system_prompt: Optional custom system prompt string
        """
        self.processor = processor
        self.system_prompt = system_prompt.strip()

    def build_conversation(
        self,
        instruction: str,
        image: Optional[Image.Image] = None,
        answer: Optional[str] = "",
    ) -> List[Dict[str, Union[str, List[Dict[str, Any]]]]]:
        """
        Build a conversation in Qwen-VL format.

        Args:
            instruction (str): Task instruction
            image (Optional): PIL image object or None placeholder
            answer (str): GPT's answer; empty string for inference

        Returns:
            A list of conversation messages
        """
        conversation = []

        if self.system_prompt:
            conversation.append({
                "role": "system",
                "content": self.system_prompt
            })

        conversation.append({
            "role": "user",
            "content": [
                {"type": "text", "text": f"What action should the robot take to {instruction.strip().lower()}?"},
                {"type": "image", "image": image}  # â† usually None before actual processing
            ]
        })

        conversation.append({
            "role": "gpt",
            "content": answer if answer else ""
        })

        return conversation

    def build_multimodal_inputs(
        self,
        conversation: List[Dict[str, Any]],
        image: Image.Image,
        add_generation_prompt: bool = True,
        return_prompt_text: bool = False,
        ) -> Union[Dict[str, Any], tuple]:
        """
        Apply Qwen chat template and tokenize with image.

        Args:
            multimodal_input (List[Dict]): The structured conversation (text + image placeholder).
            image (PIL.Image): Image input (resized inside).
            add_generation_prompt (bool): Whether to append <|im_start|>assistant for generation.
            return_prompt_text (bool): Whether to return raw string prompt.

        Returns:
            inputs (Dict[str, Tensor]) or (inputs, prompt_text)
        """
        prompt_text = self.processor.apply_chat_template(
            conversation,
            tokenize=False,
            add_generation_prompt=add_generation_prompt
        )

        image = image.resize((224, 224))  # Resize to Qwen-VL default input
        inputs = self.processor(
            text=[prompt_text],
            images=[image],
            padding=True,
            return_tensors="pt"
        )

        return (inputs, prompt_text) if return_prompt_text else inputs

from typing import Optional, Union, Dict, Any, List
from PIL import Image


class QwenVLPromptHelper:
    def __init__(
        self,
        processor,
        system_prompt: str = "You are a helpful assistant."
    ):
        """
        ç”¨äºæ„å»º Qwen-VL å®šä½ç±»ä»»åŠ¡è¾“å…¥ï¼ˆåæ ‡ç‚¹å‡»/å…³é”®ç‚¹å®šä½ç­‰ï¼‰ã€‚

        Args:
            processor: Qwen çš„ AutoProcessor å®ä¾‹
            system_prompt: System æŒ‡ä»¤ï¼Œå¯æŒ‡å®šä¸ºç”¨äº JSON æ ¼å¼è¾“å‡ºç­‰ä»»åŠ¡
        """
        self.processor = processor
        self.system_prompt = system_prompt.strip()
        self.cognition_emoj = "ğŸ”"
        self.cognition_token_ids = self.processor.tokenizer("ğŸ”", add_special_tokens=False)["input_ids"][0]
    def build_conversation(
        self,
        instruction: str,
        image: Optional[Image.Image] = None,
        answer: Optional[str] = "",
        output_format: str = "coord"
    ) -> List[Dict[str, Union[str, List[Dict[str, Any]]]]]:
        """
        æ„å»ºç¬¦åˆ Qwen-VL å¤šæ¨¡æ€æ ¼å¼çš„å¯¹è¯å†…å®¹ã€‚

        Args:
            instruction: æ–‡æœ¬æŒ‡ä»¤ï¼Œå¦‚ â€œè¯·æŒ‡å‡ºçº¢è‰²æŒ‰é’®çš„ä½ç½®â€
            image: è¾“å…¥å›¾åƒå¯¹è±¡ï¼ˆå¯ä»¥ä¸º None å ä½ï¼‰
            answer: è‹¥ç”¨äºè®­ç»ƒåˆ™æä¾›çœŸå®ç­”æ¡ˆï¼ˆå¦‚ "ç‚¹å‡»ç‚¹ä¸ºï¼š[120, 256]"ï¼‰
            output_format: è¾“å‡ºæ ¼å¼ï¼ˆ"coord"=çº¯åæ ‡, "json"=ç»“æ„åŒ–, "raw"=è‡ªç”±æ–‡æœ¬ï¼‰

        Returns:
            conversation (List): å¤šæ¨¡æ€å¯¹è¯æ ¼å¼ï¼Œç”¨äº apply_chat_template
        """
        conversation = []

        if self.system_prompt:
            if output_format == "json":
                system = self.system_prompt + " Respond with only JSON format: {\"point\": [x, y]}"
            else:
                system = self.system_prompt
            conversation.append({"role": "system", "content": system})

        # ç”¨æˆ·æŒ‡ä»¤
        user_msg = [
            {"type": "image", "image": image},
            {"type": "text", "text": f"{instruction.strip().lower()}."},
            {"type": "text", "text": f"{self.cognition_emoj}"}, #TODO add more grounding here
            
        ]
        conversation.append({"role": "user", "content": user_msg})

        # GPTå›å¤ï¼ˆç”¨äºè®­ç»ƒæ—¶æä¾›æ­£ç¡®ç­”æ¡ˆï¼›æ¨ç†æ—¶ä¸ºç©ºï¼‰
        conversation.append({"role": "gpt", "content": answer if answer else ""})

        return conversation

    def build_multimodal_inputs(
        self,
        conversation: List[Dict[str, Any]],
        image: Image.Image,
        add_generation_prompt: bool = True,
        return_prompt_text: bool = False,
    ) -> Union[Dict[str, Any], tuple]:
        """
        å°†å¯¹è¯ç»“æ„è½¬æ¢ä¸ºæ¨¡å‹å¯ç”¨çš„è¾“å…¥ï¼ˆç”¨äº tokenizer + imageï¼‰ã€‚

        Args:
            conversation: å¯¹è¯å†…å®¹ï¼Œç”± build_conversation æ„é€ 
            image: PIL image è¾“å…¥
            add_generation_prompt: æ˜¯å¦æ·»åŠ  assistant å›ç­”èµ·å§‹æ ‡å¿—
            return_prompt_text: æ˜¯å¦è¿”å›åŸå§‹æ–‡æœ¬ prompt

        Returns:
            model_inputs (Dict)ï¼Œæˆ– (model_inputs, prompt_text)
        """
        prompt_text = self.processor.apply_chat_template(
            conversation,
            tokenize=False,
            add_generation_prompt=add_generation_prompt
        )

        image = image.resize((224, 224))  # RLDS é»˜è®¤åˆ†è¾¨ç‡
        inputs = self.processor(
            text=[prompt_text],
            images=[image],
            padding=True,
            return_tensors="pt"
        )

        return (inputs, prompt_text) if return_prompt_text else inputs

    def encode_inputs(
        self,
        instruction: str,
        image: Image.Image,
        answer: Optional[str] = "",
        output_format: str = "coord",
        return_prompt_text: bool = False,
        add_generation_prompt: bool = True
    ) -> Union[Dict[str, Any], tuple]:
        """
        é«˜çº§å°è£…ï¼Œä¸€æ­¥æ„é€  prompt + tokenizer è¾“å‡ºã€‚

        Args:
            instruction: ä»»åŠ¡æŒ‡ä»¤ï¼Œå¦‚ â€œæŠ“ä½é‚£ä¸ªçº¢è‰²çš„ç“¶å­â€
            image: PIL image è¾“å…¥
            answer: è‹¥ä¸ºè®­ç»ƒåˆ™æä¾› ground truth
            output_format: â€œcoordâ€ / â€œjsonâ€ / â€œrawâ€
            return_prompt_text: æ˜¯å¦è¿”å›åŸå§‹æ–‡æœ¬ prompt
            add_generation_prompt: æ˜¯å¦æ·»åŠ ç”Ÿæˆæç¤ºç¬¦

        Returns:
            Dict æˆ– (Dict, str): inputs ä»¥åŠ prompt_textï¼ˆå¯é€‰ï¼‰
        """
        conv = self.build_conversation(
            instruction=instruction,
            image=None,  # å¯¹è¯ä¸­çš„å ä½ï¼ŒçœŸå® image åœ¨ tokenizer è¾“å…¥ä¸­æä¾›
            answer=answer,
            output_format=output_format
        )
        return self.build_multimodal_inputs(
            conversation=conv,
            image=image,
            add_generation_prompt=add_generation_prompt,
            return_prompt_text=return_prompt_text
        )

if __name__ == "__main__":
    from PIL import Image
    from modelscope import AutoProcessor

    # åŠ è½½ Qwen-VL å¤„ç†å™¨
    processor = AutoProcessor.from_pretrained("Qwen/Qwen-VL-Chat")

    helper = QwenVLPromptHelper(processor)

    img = Image.open("example.jpg")
    instruction = "press the red button"

    inputs, prompt = helper.encode_inputs(
        instruction=instruction,
        image=img,
        output_format="json",  # æˆ– "coord"
        return_prompt_text=True
    )

    print(prompt)
    # -> ä¼šè¾“å‡ºå®Œæ•´ promptï¼ŒåŒ…æ‹¬ system/user/assistant ç»“æ„
    # -> inputs å¯ç›´æ¥ä¼ å…¥ model.generate()
