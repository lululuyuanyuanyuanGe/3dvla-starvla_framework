"""
cogactvla.py

"""

from __future__ import annotations

from functools import partial
from pathlib import Path
from typing import Callable, Dict, List, Optional, Type, Union, Tuple
from copy import deepcopy

import torch
import torch.nn as nn
import numpy as np
from PIL import Image
from torch.distributed.fsdp.wrap import _module_wrap_policy, _or_policy
from torch.nn.utils.rnn import pad_sequence
from transformers.modeling_outputs import CausalLMOutputWithPast
from transformers import LlamaTokenizerFast
import re
from prismatic.models.backbones.llm import LLMBackbone
from prismatic.models.backbones.llm.prompting import PromptBuilder
from prismatic.models.backbones.vision import VisionBackbone
from prismatic.models.vlms.base_vlm import VLM
from prismatic.models.vlms.prismatic import PrismaticVLM
from prismatic.overwatch import initialize_overwatch

from llavavla.model.action_model.action_model import ActionModel
from llavavla.model.action_model.models import DiT
from llavavla.dataloader.promt_builder import QwenVLPromptHelper
import torch.distributed as dist
from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor, AutoConfig
from qwen_vl_utils import process_vision_info
# Initialize Overwatch =>> Wraps `logging.Logger`
overwatch = initialize_overwatch(__name__)


# HuggingFace Default / LLaMa-2 IGNORE_INDEX (for labels)
IGNORE_INDEX = -100


# get QWen2.5
from llavavla.model.vlm import _QWen_VL_Interface #ä¸åº”è¯¥å¼ºä¾èµ–äºŽè¿™ä¸ªï¼Œåº”è¯¥æ˜¯ä¸€ä¸ªæŽ¥å£ç±»ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªå…·ä½“çš„ç±», TODO ä¸è¦å®žçŽ° hard æŽ¥å£ç±»ï¼Œ ä½¿ç”¨ **kwargs
from llavavla.model.tools import auto_get_module_keys, auto_get_trainable_modules


class CogACT_Qwen(nn.Module):
    def __init__(
        self,
        vlm:_QWen_VL_Interface, # è¿™æ˜¯ä¸å¥½çš„å®žçŽ°ï¼Œ ä¸€å®šä¸èƒ½æ˜¯äº’ç›¸ä¾èµ–
        action_model_type: str = 'DiT-B',
        token_size: int = 2048,
        action_dim: int = 7,
        future_action_window_size: int = 15,
        past_action_window_size: int = 0,
        use_ema: bool = False,
        norm_stats: Dict[str, Dict[str, Dict[str, Dict[str, List[float]]]]] = None,
        **kwargs,
    ) -> None:
        super().__init__()
        
        self.action_model = ActionModel(model_type = action_model_type,  # TODO @Jinhui åº”è¯¥å†™åˆ° get_action_model()
                                            action_hidden_dim = token_size, 
                                            in_channels = action_dim, 
                                            future_action_window_size = future_action_window_size, 
                                            past_action_window_size = past_action_window_size)
        self.vlm = vlm

        # print("Freezing QWEN-VL model parameters") # @Jinhui Bug, åº”è¯¥ç»Ÿä¸€ä½ç½®æ¥ è®¾ç½®è¿™ä¸ª
        # for param in self.vlm.parameters():
        #     param.requires_grad = False
        
        self.qwen_processor = vlm.processor # 
        self.future_action_window_size = future_action_window_size
        self.past_action_window_size = past_action_window_size
        self.use_ema = use_ema
        if self.use_ema:
            self.ema_diffusion = deepcopy(self.action_model)
            self.ema_diffusion.requires_grad_(False)
        #     self.all_module_keys = ['action_model', 'ema_diffusion']
        # else:
        #     self.all_module_keys = ['action_model']
        
        # TODO check ä¸ºä»€ä¹ˆæ”¹æ–‡ä»¶model åå­—ä¹ˆï¼Ÿ 
        # for module_keys in self.vlm.all_module_keys: #@Jinhui checking
        #     self.all_module_keys.append("vlm." + module_keys)
        self.all_module_keys = auto_get_module_keys(self)
        # Diffusion head is always trainable
        # self._trainable_module_keys = ['action_model'] # åº”è¯¥æ”¾åˆ°ä¸€ä¸ªé›†ä¸­çš„åœ°æ–¹
        self.norm_stats = norm_stats

        # è¿™é‡Œåº”è¯¥å’Œ data loader tranfomation å¯¹é½çš„
        self.promptHelper = QwenVLPromptHelper(processor=self.vlm.processor, system_prompt="You are a helpful assistant")

    @property
    def trainable_module_keys(self) -> List[str]:
        # keys = []
        # for module_keys in self.vlm.trainable_module_keys:
        #     keys.append("vlm." + module_keys)
        # keys += self._trainable_module_keys
        # TODO check, åŽŸç‰ˆè¿”å›žçš„æ­» vlm.model, æ–°çš„å®žçŽ°æ˜¯vlm --> çœ‹ä¸€ä¸‹ä¿å­˜é€»è¾‘æ˜¯å¦å‘ä¸Šå˜åŒ–
        keys = auto_get_trainable_modules(self, max_depth=1)# auto åŽ»åˆ¤æ–­å“ªäº›moduleæ˜¯trainableçš„
        return keys
    
    
    @staticmethod
    def align_module_names(state_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """Aligns module names in the state dict to match the current model's module names."""
        # TODO æ˜¯ä¸€ä¸ªä¸´æ—¶æ–¹æ³•ï¼ŒåŽæœŸè¦è€ƒè™‘align save çš„é€»è¾‘ï¼Œè®©ä»–èƒ½å¤Ÿç›´æŽ¥load è¿›æ¥
        """
        Align the keys in the state_dict to match the expected model structure.

        Args:
            state_dict (dict): Original model state_dict with misaligned keys.

        Returns:
            dict: Aligned state_dict.
        """
        aligned_dict = {}

        # Step 1: å¤„ç† `model`ï¼Œé‡å‘½åä¸º `vlm.model`
        if "model" in state_dict:
            for key, value in state_dict["model"].items():
                aligned_dict[f"vlm.model.{key}"] = value  # æ·»åŠ å‰ç¼€

        # Step 2: åˆ é™¤ `visual` å’Œ `lm_head`ï¼ˆå‡è®¾å®ƒä»¬ä¸ºç©ºï¼‰
        if "visual" in state_dict:
            if state_dict["visual"]:  # å¦‚æžœ visual ä¸æ˜¯ç©ºçš„ï¼Œå¯èƒ½éœ€è¦å¤„ç†
                print("Warning: 'visual' is expected to be empty but contains data.")
            del state_dict["visual"]

        if "lm_head" in state_dict:
            if state_dict["lm_head"]:  # å¦‚æžœ lm_head ä¸æ˜¯ç©ºçš„ï¼Œå¯èƒ½éœ€è¦å¤„ç†
                print("Warning: 'lm_head' is expected to be empty but contains data.")
            del state_dict["lm_head"]

        return aligned_dict

    def freeze_backbones(self, stage):
        # self.vlm.freeze_backbones(stage)

        """
        æ ¹æ®ç»™å®šçš„æ­£åˆ™æ¨¡å¼åˆ—è¡¨å†»ç»“æ¨¡å—ã€‚
        å¦‚æžœæŸä¸ªæ¨¡å—çš„åç§°åŒ¹é…ï¼ˆä¾‹å¦‚å…¬å…±å‰ç¼€åŒ¹é…ï¼‰ï¼Œåˆ™å†»ç»“è¯¥æ¨¡å—ä¸‹æ‰€æœ‰å‚æ•°ï¼ˆä¸å†é€’å½’å†»ç»“å­æ¨¡å—ï¼‰ï¼Œ
        å¹¶è¿”å›žå†»ç»“æ¨¡å—åç§°çš„æœ‰åºæµ…å±‚åˆ—è¡¨ã€‚
        
        å‚æ•°ï¼š
            patterns: æ­£åˆ™è¡¨è¾¾å¼å­—ç¬¦ä¸²åˆ—è¡¨ï¼Œæ¨¡å—åç§°åªè¦åŒ¹é…å…¶ä¸­ä¸€ä¸ªæ¨¡å¼ï¼Œå°±ä¼šè¢«å†»ç»“ã€‚
            
        è¿”å›žï¼š
            ä¸€ä¸ªå†»ç»“æ¨¡å—åç§°çš„åˆ—è¡¨ï¼ˆæŒ‰é€’å½’é¡ºåºï¼‰ã€‚
        """
        # r"^vlm\.model\.visual", r"^action_model"
        patterns = [] #TODO æ—¶å€™è¦å‚æ•°åŒ–
        def freeze_module(module: nn.Module, prefix: str) -> List[str]:
            # å¦‚æžœå½“å‰æ¨¡å—åç§°åŒ¹é…ä»»ä¸€æ¨¡å¼ï¼Œåˆ™å†»ç»“å½“å‰æ¨¡å—ï¼Œä¸å†é€’å½’å­æ¨¡å—
            if any(re.match(pattern, prefix) for pattern in patterns if prefix):
                for param in module.parameters(recurse=False):
                    param.requires_grad = False
                return [prefix]
            # å¦åˆ™ï¼Œé€’å½’éåŽ†å­æ¨¡å—
            frozen_keys = []
            for name, child in module.named_children():
                full_name = f"{prefix}.{name}" if prefix else name
                frozen_keys.extend(freeze_module(child, full_name))
                
            return frozen_keys
        
        # å¯¹æ•´ä¸ªæ¨¡å—ï¼ˆselfï¼‰é€’å½’æ£€æŸ¥ã€‚æ³¨æ„ï¼Œæ ¹ç›®å½•é€šå¸¸ä¸ºç©ºå­—ç¬¦ä¸²ï¼Œè¿™é‡Œä¸å†»ç»“æ ¹æ¨¡å—
        frozen = []
        for name, child in self.named_children():
            full_name = name  # é¡¶å±‚æ¨¡å—åç§°
            frozen.extend(freeze_module(child, full_name))
        return frozen


    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        pixel_values: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        actions: Optional[torch.FloatTensor] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        past_key_values: Optional[List[torch.FloatTensor]] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        repeated_diffusion_steps: int = 4,
        action_masks = None,
        **kwargs,  # ðŸ‘ˆ æ•æ·ä»£ç çš„çµæ´»æ€§ï¼Œ å…è®¸ä»»ä½•å½¢å¼çš„ä¼ å‚æ•°
    ) -> Tuple:
        """Run a forward pass through the VLM, returning a CausalLMOutputWithPast instance (contains loss)."""
        # @Jinhui TBD TODO 
        # pixel_values = pixel_values["pixel_values"] # labeles = pixel_values["labels"]
        # dist.barrier()
        output: CausalLMOutputWithPast = self.vlm( #system 
            input_ids=input_ids,
            image_grid_thw=kwargs.get("image_grid_thw", None),  # å¯èƒ½æ˜¯ä¸€ä¸ªå›¾åƒç½‘æ ¼
            attention_mask=attention_mask,
            pixel_values=pixel_values,
            labels=labels, # label å…¨æ˜¯ -100 @Jinhui TODO Bug here, input ä¹Ÿå…¨æ˜¯ä¸€æ ·çš„
            inputs_embeds=inputs_embeds,
            past_key_values=past_key_values,
            use_cache=use_cache,
            output_attentions=False,
            output_hidden_states=True,
            return_dict=True,
        )
        vlm_loss = output.loss # @Jinhui TODO è¿™é‡Œæ˜¯å¯ä»¥study çš„åœ°æ–¹ï¼Œ æ˜¯å¦ training lang
        # extract the last hidden state and the learnable EOS token feature
        last_hidden = output.hidden_states[-1] # B,len,D
        cognition_features = self._get_cognition_features(last_hidden, input_ids, attention_mask=attention_mask)


        actions_history = actions[:,0:self.past_action_window_size,:]
        actions_future = actions[:, -(self.future_action_window_size+1):, :]
        
        # Repeat 'actions' 'repeated_diffusion_steps' times, resulting in [repeated_diffusion_steps*B, T, D]
        actions_repeated = actions_future.repeat(repeated_diffusion_steps, 1, 1)
        actions_history_repeated = actions_history.repeat(repeated_diffusion_steps, 1, 1)
        cognition_features_repeated = cognition_features.repeat(repeated_diffusion_steps, 1, 1) # [repeated_diffusion_steps*B, 1, D]

        # Action model forward and compute loss
        loss = self.action_model.loss(actions_repeated, cognition_features_repeated) # TODO loss åº”è¯¥æ”¾åˆ°å¦ä¸€ä¸ªå‡½æ•°
        return loss, output

    def _get_cognition_features_old(self, last_hidden, input_ids) -> torch.Tensor:
        # last_hidden = outputs.hidden_states[-1] # B,len,D

        # extract the visual token number #@Jinhui ä»–è¦æ‹¿ä¸€ä¸ªtoken åŽ»åš ä¸‹æ¸¸ TODO å±•ç¤ºä¸è¦ç”¨å…³
        # @Discussion è¿™ä¸ªä½ç½®éœ€è¦è®¨è®º --> å…¶å®žå¯ä»¥é€šè¿‡æ£€æŸ¥ visual token çš„ indexs æ¥
        image_token_id, vido_token_id, pad_token_id = 151655, 151654, 151643
        # 1ï¸âƒ£ æ‰¾åˆ°ç¬¬ä¸€ä¸ªåŒ¹é…çš„ image_token_id çš„ç´¢å¼•
        match_indices = (input_ids == image_token_id).float().argmax(dim=1)  # [B]

        # 2ï¸âƒ£ ç¡®ä¿ç´¢å¼•ä¸ä¼šè¶Šç•Œï¼ˆå‰ä¸€ä¸ª token ä¸èƒ½æ˜¯ -1ï¼‰# ä¸å¯¹çš„ï¼Œå› ä¸ºæ²¡æœ‰causal attention è¿˜æ²¡çœ‹åˆ° image tokenï¼Œ åº”è¯¥æ˜¯next token
        prev_indices = torch.clamp(match_indices - 1, min=0)  # [B] 

        # 3ï¸âƒ£ æ‰©å±•ç´¢å¼•ï¼Œä½¿å…¶åŒ¹é… last_hidden ç»´åº¦
        expanded_indices = prev_indices.unsqueeze(-1).expand(-1, last_hidden.size(-1))  # [B, D]

        # 4ï¸âƒ£ æå– cognition_features (å‰ä¸€ä¸ª token çš„éšè—çŠ¶æ€)
        cognition_features = last_hidden.gather(1, expanded_indices.unsqueeze(1))  # [B, 1, D]
        cognition_features = cognition_features.to(torch.bfloat16)
        
        return cognition_features
    def _get_cognition_features(self, last_hidden: torch.Tensor, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:
        """
        æå–æ¯ä¸ªæ ·æœ¬ä¸­ cognition_tokenï¼ˆðŸ”ï¼‰ä½ç½®çš„ hidden stateï¼Œä½œä¸º cognition featureã€‚

        Args:
            last_hidden: Tensor, shape [B, T, D]
            input_ids: Tensor, shape [B, T]
            attention_mask: Tensor, shape [B, T]

        Returns:
            cognition_features: Tensor, shape [B, 1, D]
        """
        cognition_token_id = self.promptHelper.cognition_token_ids

        B, T, D = last_hidden.shape

        # æ‰¾åˆ°æ¯ä¸ªæ ·æœ¬ä¸­ cognition_token_id å‡ºçŽ°çš„ä½ç½®
        cognition_indices = (input_ids == cognition_token_id).int()  # [B, T]ï¼Œä¸º1çš„ä½ç½®æ˜¯ cognition
        has_cognition_token = cognition_indices.any(dim=1)

        if not has_cognition_token.all():
            raise ValueError("Not all samples contain the cognition token ðŸ”")

        # èŽ·å–æ¯è¡Œ cognition_token çš„ä½ç½®ï¼ˆåªå–ç¬¬ä¸€ä¸ªåŒ¹é…çš„ token indexï¼‰
        cognition_pos = cognition_indices.argmax(dim=1)  # [B]
        
        # æž„å»ºç”¨äºŽ gather çš„ç´¢å¼•
        gather_index = cognition_pos.unsqueeze(1).unsqueeze(2).expand(-1, 1, D)  # [B, 1, D]
        cognition_features = last_hidden.gather(dim=1, index=gather_index)  # [B, 1, D]

        return cognition_features

    
    def get_fsdp_wrapping_policy(self) -> Callable: 
        """Return an FSDP _or_policy over the policies returned by each individual backbone (and our VLM policy)."""
        # vision_fsdp_wrapping_policy = self.vlm.vision_backbone.get_fsdp_wrapping_policy()
        # llm_fsdp_wrapping_policy = self.vlm.llm_backbone.get_fsdp_wrapping_policy()
        vlm_fsdp_wrapping_policy = self.vlm.get_fsdp_wrapping_policy()

        # Get Prismatic Wrapping Policy =>> just a module wrapping policy around `self.projector` and DiT
        prismatic_fsdp_wrapping_policy = partial(
            _module_wrap_policy,
            module_classes={DiT},
        )

        # Return union (_or_) over constituent policies
        #   => Note: there is *not* a fall-through policy; any module that isn't covered by the above constituents will
        #            automatically be folded into the root VLM FSDP instance.
        return partial(
            _or_policy,
            policies=[
                vlm_fsdp_wrapping_policy,
                prismatic_fsdp_wrapping_policy, # @Jinhui TODO Checkingè¿™ä¸ªåº”è¯¥ä¿ç•™ä¹ˆï¼Ÿ
            ],
        )

    def load_ema_to_weights(self):
        """Load the EMA state dict to the weights."""
        if self.use_ema:
            self.action_model.load_state_dict(self.ema_diffusion.state_dict())
            del self.ema_diffusion

    @classmethod
    def from_pretrained(
        cls,
        pretrained_checkpoint: Path,
        base_vlm: str,
        freeze_weights: bool = True,
        action_dim: int = 7,
        future_action_window_size: int = 15,
        past_action_window_size: int = 0,
        action_model_type: str = 'DiT-B',
        use_ema: bool = False,
        norm_stats = None,
        **kwargs,
    ) -> CogACT_Qwen:

        # Load VLM backbone, borrowed from PrismaticVLM

        # ä»…åŠ è½½æ¨¡åž‹é…ç½®ï¼Œè€Œä¸åŠ è½½æƒé‡
        base_vlm = "/mnt/petrelfs/yejinhui/Projects/llavavla/playground/Pretrained_models/Qwen2.5-VL-3B-Instruct" # Jinhui Can be a bug TODO éœ€è¦è°ƒæ•´training å’Œæµ‹è¯•çš„å·¥ä½œç›®å½•
        # åªåˆå§‹åŒ–æ¨¡åž‹ç»“æž„ï¼Œä¸åŠ è½½å‚æ•°
        vlm = _QWen_VL_Interface(model_id=base_vlm, load_for_training=False)
   
        

        # Load from Checkpoint (Custom --> should load both *projector* and *llm* weights)
        model_state_dict = torch.load(pretrained_checkpoint, map_location="cpu") #["model"]
        # qwen_state_dict = QwenACT_state_dict["model"]
        # Initialize CogACT
        cogact = CogACT_Qwen(vlm,
                        token_size = vlm.model.config.hidden_size, # è¿™é‡Œçš„ model çš„åˆ†æˆå¾ˆå¥‡æ€ª
                        action_dim = action_dim,
                        future_action_window_size = future_action_window_size,
                        past_action_window_size = past_action_window_size,
                        action_model_type = action_model_type,
                        use_ema = use_ema,
                        norm_stats = norm_stats,
                        )
        cogact.qwen_processor = vlm.processor # @Jinhui TODO ä¸ºä»€ä¹ˆä¸æ”¾åˆ° inital
        # Load VLM from Checkpoint # TODO åŽæœŸè¦å¯¹é½ save çš„é€»è¾‘
        # qwen_state_dict = CogACT_Qwen.align_module_names(model_state_dict)
        # assert CogACT_Qwen.check_unexpected_keys(qwen_state_dict,cogact),  "check_point ä¸­æœ‰å‚æ•°æ²¡æœ‰è¢« load"
        # cogact.vlm.load_state_dict(model_state_dict["vlm"])  # @Jinhui ä»»åŠ¡æ•´ä¸ªmodelä¸€èµ·ï¼Œé€»è¾‘å†™åˆ°é‡Œé¢é‡Œé¢
 
        # è‡ªåŠ¨åŠ è½½ checkpoint ä¸­çš„æƒé‡åˆ°å¯¹åº”æ¨¡å— #@Jinhui TODO æ€Žä¹ˆä¿è¯å…¨éƒ¨trainableå‚æ•°è¢«save äº†ï¼Ÿ
        # éåŽ† checkpoint ä¸­çš„æ¯ä¸ªé”®ï¼Œè‹¥ cogact æœ‰ç›¸åº”å±žæ€§ä¸”è¯¥å±žæ€§æ”¯æŒ load_state_dictï¼Œåˆ™åŠ è½½æƒé‡
        model_keys = cogact.state_dict().keys()

        for key, state in model_state_dict.items():
            if key in model_keys:
                try:
                    cogact.state_dict()[key].copy_(state)
                    # overwatch.info(f"âœ… Successfully loaded weights for key '{key}'")
                except Exception as e:
                    overwatch.warning(f"âš ï¸ Failed to copy weight for key '{key}': {e}")
            else:
                overwatch.warning(f"âš ï¸ Unknown key '{key}' in checkpoint. Ignoring.")
        
        # TODO éœ€è¦ä¸€ä¸ªé€»è¾‘æ£€æŸ¥æ˜¯å¦å…¨éƒ¨å‚æ•°å°±load å¥½äº†ï¼Ÿ --> ä¸ç›´æŽ¥ cogact.load çš„åŽŸå› 
        # TODO Jinhui å¾ˆå¿…è¦æœ‰ä¸ªæ£€æŸ¥æµç¨‹ï¼Œä¿è¯ all tranable å‚æ•°è¢« save äº†, all tranable  è¢«load äº†
        return cogact

    @torch.inference_mode()
    def predict_action(
        self, image: Image, 
        instruction: str, 
        unnorm_key: Optional[str] = None, 
        cfg_scale: float = 1.5, 
        use_ddim: bool = False,
        num_ddim_steps: int = 5,
        **kwargs: str
    ) -> np.ndarray:
        """
        Core function for VLA inference; maps input image and task instruction to continuous action.

        @param image: PIL Image as [height, width, 3]
        @param instruction: Task instruction string
        @param unnorm_key: Optional dataset name for retrieving un-normalizing statistics; if None, checks that model
                           was trained only on a single dataset, and retrieves those statistics.
        @param cfg_scale: Scaling factor for classifier-free guidance (CFG); if == 1.0, CFG is disabled.
        @param use_ddim: Use DDIM sampling instead of DDPM sampling.
        @param num_ddim_steps: Number of DDIM steps to use for sampling.

        @return Unnormalized (continuous) action vector --> end-effector deltas.
        """
        # qwen_processor = self.qwen_processor
        #@Jinhui æƒ³åŠžæ³•æ€Žä¹ˆå’Œdataloader å¯¹é½ï¼Ÿ
        # Build VLA Prompt
        # prompt_builder = self.vlm.get_prompt_builder()
        # prompt_builder.add_turn(role="human", message=f"What action should the robot take to {instruction.lower()}?")

        # TODO ä¸ºäº†ä¿è¯æµ‹è¯•ä¸€è‡´æ€§å¿ƒé‡Œåº”è¯¥æ˜¯ ç”¨func, ä½†æ˜¯å¦‚æžœé¢„æœŸè¿™é‡Œæ˜¯ template-free, å°±åº”è¯¥æ˜¯è¿™æ ·çš„
        # minin version of QwenPromptBuilder --> @Jinhui TODO åŽç»­å¯ä»¥å®žçŽ°åˆ° QwenPromptBuilder ä¸­è¿›è¡Œå¯¹è¯ç®¡ç†
        # æ‹¿åˆ° å¯¹è¯çš„ text æ–‡æœ¬ 
        # conversation = [
        #     {"role": "user", "content": [{"type": "text", "text":f"What action should the robot take to {instruction.lower()}?"}, {"image": None}]},
        #     ]
                
        # @ä¹‹åŽå†™å…¥æ¨¡åž‹å†…éƒ¨ï¼Œ å˜æˆç§æœ‰åŒ–æ–¹æ³•
        # img = Image.fromarray(rlds_batch["observation"]["image_primary"][0]) # B è¦è¢«åŽ»æŽ‰ï¼Ÿ
        # image resize to 224*224
        img = image.resize((224, 224))  # Resize to Qwen-VL default input size
        lang = instruction.lower() + "ðŸ”" #cognition token
        messages = [
        {
            "role": "user",
            "content": [
                {"type": "image", "image": img}, # rgb
                {"type": "text", "text": lang},
            ],
        },]
        text = self.qwen_processor.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=False
        )
        image_inputs, video_inputs = process_vision_info(messages)
        inputs = self.qwen_processor(
            text=text,
            images=image_inputs,
            videos=video_inputs,
            padding=True,
            return_tensors="pt",
        )



        # Invoke super().generate --> taps into `GenerationMixin` which (redirects) to `forward()`
        autocast_dtype = self.vlm.half_precision_dtype # ä¸ºä»€ä¹ˆç”¨åŠç²¾åº¦æŽ¨ç†
        
        # add by Jinhui
        device = self.vlm.device  # ç¡®ä¿æ‰€æœ‰è¾“å…¥å’Œæ¨¡åž‹éƒ½åœ¨åŒä¸€è®¾å¤‡ä¸Š
        dtype = next(self.vlm.parameters()).dtype  # èŽ·å–æ¨¡åž‹çš„é»˜è®¤æ•°æ®ç±»åž‹ï¼ˆfloat16 æˆ– float32ï¼‰

        
        # ç¡®ä¿æ‰€æœ‰å¼ é‡éƒ½ç§»åŠ¨åˆ° GPUï¼Œå¹¶è½¬æ¢ä¸ºæ­£ç¡®çš„æ•°æ®ç±»åž‹
        for key in inputs:
            if isinstance(inputs[key], torch.Tensor):
                if key in ["input_ids", "attention_mask"]:  # ä¿è¯ input_ids å’Œ attention_mask ä»ç„¶æ˜¯ long ç±»åž‹
                    inputs[key] = inputs[key].to(device, dtype=torch.long)
                elif key in ["image_grid_thw"]:
                    continue
                else:  # å…¶ä»– Tensorï¼ˆå¦‚ pixel_valuesï¼‰è½¬æ¢æˆæ¨¡åž‹ dtypeï¼ˆfloat16 æˆ– float32ï¼‰
                    inputs[key] = inputs[key].to(device, dtype=dtype)


        # end add by Jinhui
        # Generate cognition feature through vlm
        with torch.autocast("cuda", dtype=autocast_dtype, enabled=self.vlm.enable_mixed_precision_training):
            # fmt: off
            outputs = self.vlm(
                **inputs,
                output_hidden_states=True, 
                return_dict=True,
            ) # generation æ‹¿ä¸åˆ°å‰é¢token çš„ä¿¡æ¯ï¼Œè€ƒè™‘ä½¿ç”¨ forward?

            # Jinhui see text # outputs.sequences.shape: B, len with prefix
            # outputs.input_ids = outputs.sequences # ä¸ºäº†å’Œ input dict ä¿æŒä¸€è‡´ï¼Œ æ–¹ä¾¿è°ƒç”¨ self._get_cognition_features# è¿˜çœŸä¸å¤ªä¸€æ ·ï¼Œå› ä¸ºgenerationçš„é€»è¾‘å’Œ forwardä¸ä¸€æ ·
            # generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, outputs.sequences)]
            # output_text = self.qwen_processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)
            # print("output:\n",output_text[0])
            # fmt: on
            # æˆ‘ä»¬trainingçš„æ—¶å€™æ˜¯ image ä¸å›ºå®šåœ¨æœ€å‰é¢æ²¡ï¼Œæ˜¯æ²¡åŠžæ³•åªmax_new = 1 çš„
        # Extract cognition feature
        # outputs.hidden_states = list = next tokens 
        # be careful about the where the cognition_features comes from would align with training
        # cognition_features = output.hidden_states[0][-1][:,-1,:]  # nexx tokens, layers, B, len, D #@Jinhui to Think è¿™é‡Œä¸ºä»€ä¹ˆæ¯ä¸€ä¸ª next token éƒ½è®°å½•äº† å…¨éƒ¨éƒ½ hidden? ä¸æ˜¯çš„ï¼Œåªæœ‰ç¬¬ä¸€ä¸ªä¼š
        last_hidden_states = outputs.hidden_states[-1] #torch.Size([1, 428, 2048]) # last hidden_states for next token generation
        cognition_features = self._get_cognition_features(last_hidden_states, inputs.input_ids, attention_mask=inputs.attention_mask) # [B,1,D] TODO carefully checking with align training

        assert (cognition_features.shape[0], cognition_features.shape[1], cognition_features.shape[-1]) == (1, 1,2048), "Batch size must be 1 for action prediction"
        using_cfg = cfg_scale > 1.0

        model_dtype = next(self.action_model.net.parameters()).dtype
        B = cognition_features.shape[0]

        # Sample random noise
        noise = torch.randn(B, self.future_action_window_size+1, self.action_model.in_channels, device=cognition_features.device).to(model_dtype)  #[B, T, D]
    
        # Setup classifier-free guidance:
        if using_cfg:
            noise = torch.cat([noise, noise], 0)
            uncondition = self.action_model.net.z_embedder.uncondition
            uncondition = uncondition.unsqueeze(0)  #[1, D]
            uncondition = uncondition.expand(B, 1, -1) #[B, 1, D]
            z = torch.cat([cognition_features, uncondition], 0)
            cfg_scale = cfg_scale
            model_kwargs = dict(z=z, cfg_scale=cfg_scale)
            sample_fn = self.action_model.net.forward_with_cfg
        else:
            model_kwargs = dict(z=cognition_features)
            sample_fn = self.action_model.net.forward

        # DDIM Sampling
        if use_ddim and num_ddim_steps is not None:
            if self.action_model.ddim_diffusion is None:
                self.action_model.create_ddim(ddim_step=num_ddim_steps)
            samples = self.action_model.ddim_diffusion.ddim_sample_loop(sample_fn, 
                                                                noise.shape, 
                                                                noise, 
                                                                clip_denoised=False,
                                                                model_kwargs=model_kwargs,
                                                                progress=False,
                                                                device=cognition_features.device,
                                                                eta=0.0
                                                                )
        else:
            # DDPM Sampling
            samples = self.action_model.diffusion.p_sample_loop(sample_fn, 
                                                                    noise.shape, 
                                                                    noise, 
                                                                    clip_denoised=False,
                                                                    model_kwargs=model_kwargs,
                                                                    progress=False,
                                                                    device=cognition_features.device
                                                                    )
        if using_cfg:
            samples, _ = samples.chunk(2, dim=0)  # Remove null class samples
        normalized_actions = samples[0].cpu().numpy()

        # Un-normalize Actions        
        action_norm_stats = self.get_action_stats(unnorm_key)
        mask = action_norm_stats.get("mask", np.ones_like(action_norm_stats["q01"], dtype=bool))
        action_high, action_low = np.array(action_norm_stats["q99"]), np.array(action_norm_stats["q01"])
        normalized_actions = np.clip(normalized_actions, -1, 1)
        normalized_actions[:, 6] = np.where(normalized_actions[:, 6] < 0.5, 0, 1) 
        actions = np.where(
            mask,
            0.5 * (normalized_actions + 1) * (action_high - action_low) + action_low,
            normalized_actions,
        )

        return actions, normalized_actions

    @torch.inference_mode()
    def predict_action_batch(
        self, image: List[Image], 
        instruction: List[str], 
        unnorm_key: Optional[str] = None, 
        cfg_scale: float = 1.5, 
        use_ddim: bool = False,
        num_ddim_steps: int = 10,
        **kwargs: str
    ) -> np.ndarray:
        """
        Core function for VLA inference in batch; maps input image and task instruction to continuous action.
        This function is used for batch inference in the simulators.
        @param image: PIL Image as [height, width, 3]
        @param instruction: Task instruction string
        @param unnorm_key: Optional dataset name for retrieving un-normalizing statistics; if None, checks that model
                           was trained only on a single dataset, and retrieves those statistics.
        @param cfg_scale: Scaling factor for classifier-free guidance (CFG); if == 1.0, CFG is disabled.
        @param use_ddim: Use DDIM sampling instead of DDPM sampling.
        @param num_ddim_steps: Number of DDIM steps to use for sampling.

        @return Unnormalized (continuous) action vector --> end-effector deltas.
        """
        image_transform, tokenizer = self.vlm.vision_backbone.image_transform, self.vlm.llm_backbone.tokenizer
        
        input_ids = []
        pixel_values = []

        # Build VLA Prompt
        B = len(image)

        if isinstance(tokenizer, LlamaTokenizerFast):
            pass
        else:
            raise ValueError(f"Unsupported `tokenizer` type = {type(tokenizer)}")

        for id in range(B):
            prompt_builder = self.vlm.get_prompt_builder()
            prompt_builder.add_turn(role="human", message=f"What action should the robot take to {instruction[id].lower()}?")
            prompt_text = prompt_builder.get_prompt()
            # Prepare Inputs
            single_input_ids = tokenizer(prompt_text, truncation=True, return_tensors="pt").input_ids.to(self.vlm.device).squeeze(0)
            # Note: We need to add this special empty token ('') after the colon (':') token in "ASSISTANT:"
            #       insert it to match the inputs seen at training time. The empty token is at index 29871.
            #       We also need to add the special cognition token at index 2 (i.e. the EOS token).
            single_input_ids = torch.cat(
                (single_input_ids, torch.Tensor([29871, 2]).long().to(self.vlm.device)), dim=0
            ) # [seq]

            input_ids.append(single_input_ids)
            # Preprocess Image
            pixel_values.append(image_transform(image[id]))

        # Padding
        padding_side = "right"
        # For now, we only support Tokenizers with `padding_side = "right"`
        #   => Handle padding via RNN Utils => `pad_sequence`
        assert padding_side == "right", f"Invalid Tokenizer `{padding_side = }`"

        model_max_length = tokenizer.model_max_length
        pad_token_id = tokenizer.pad_token_id
        input_ids = pad_sequence(input_ids, batch_first=True, padding_value=pad_token_id)

        # Truncate (if necessary)
        input_ids = input_ids[:, : model_max_length]
        # Get `attention_mask` by checking for `pad_token_id`
        attention_mask = input_ids.ne(pad_token_id)

        # Preprocess Image
        if isinstance(pixel_values[0], torch.Tensor):
            pixel_values = torch.stack(pixel_values).to(self.vlm.device)
        elif isinstance(pixel_values[0], dict):
            pixel_values = {
                k: torch.stack([pixel_values[idx][k] for idx in range(len(input_ids))]).to(self.vlm.device) for k in pixel_values[0]
            }
        else:
            raise ValueError(f"Unsupported `pixel_values` type = {type(pixel_values)}")

        # Invoke super().generate --> taps into `GenerationMixin` which (redirects) to `forward()`
        autocast_dtype = self.vlm.llm_backbone.half_precision_dtype
        with torch.autocast("cuda", dtype=autocast_dtype, enabled=self.vlm.enable_mixed_precision_training):
            # fmt: off
            output = super(PrismaticVLM, self.vlm).generate(
                input_ids=input_ids,                            # Shape: [1, seq]
                pixel_values=pixel_values,                      # Shape: [1, 3, res, res] or Dict[str, ...]
                max_new_tokens=1,
                output_hidden_states=True, 
                return_dict_in_generate=True,
                attention_mask = attention_mask,
                **kwargs
            )
            # fmt: on

        # Extract cognition feature
        if self.vlm.vision_backbone.featurizer is not None:
            num_patch = self.vlm.vision_backbone.featurizer.patch_embed.num_patches
        elif hasattr(self.vlm.vision_backbone, 'siglip_featurizer') and self.vlm.vision_backbone.siglip_featurizer is not None:
            num_patch = self.vlm.vision_backbone.siglip_featurizer.patch_embed.num_patches
        else:
            raise ValueError("No vision backbone found")

        last_hidden = output.hidden_states[0][-1]
        last_hidden = last_hidden[:, num_patch :]

        cumulative_sum = attention_mask.cumsum(dim=1)  
        last_true_indices = (cumulative_sum == cumulative_sum.max(dim=1, keepdim=True)[0]).float().argmax(dim=1)  
        expanded_indices = last_true_indices.unsqueeze(-1).expand(-1, last_hidden.size(-1))  
        cognition_features = last_hidden.gather(1, expanded_indices.unsqueeze(1)).squeeze(1) #[B, D]

        assert (cognition_features.shape[0], cognition_features.shape[1]) == (B, 4096), "Batch size must be B for action prediction"
        using_cfg = cfg_scale > 1.0


        model_dtype = next(self.action_model.net.parameters()).dtype

        B = cognition_features.shape[0]
        
        cognition_features = cognition_features.unsqueeze(1).to(model_dtype)  # [B, 1, D]

        # Sample random noise
        noise = torch.randn(B, self.future_action_window_size+1, self.action_model.in_channels, device=cognition_features.device).to(model_dtype)  #[B, T, D]
        # Setup classifier-free guidance:
        if using_cfg:
            noise = torch.cat([noise, noise], 0)
            uncondition = self.action_model.net.z_embedder.uncondition
            uncondition = uncondition.unsqueeze(0)  #[1, D]
            uncondition = uncondition.expand(B, 1, -1) #[B, 1, D]
            z = torch.cat([cognition_features, uncondition], 0)
            cfg_scale = cfg_scale
            model_kwargs = dict(z=z, cfg_scale=cfg_scale)
            sample_fn = self.action_model.net.forward_with_cfg
        else:
            model_kwargs = dict(z=cognition_features)
            sample_fn = self.action_model.net.forward

        # DDIM Sampling
        if use_ddim and num_ddim_steps is not None:
            if self.action_model.ddim_diffusion is None:
                self.action_model.create_ddim(ddim_step=num_ddim_steps)
            samples = self.action_model.ddim_diffusion.ddim_sample_loop(sample_fn, 
                                                                noise.shape, 
                                                                noise, 
                                                                clip_denoised=False,#False, try to set True 
                                                                model_kwargs=model_kwargs,
                                                                progress=False,
                                                                device=cognition_features.device,
                                                                eta=0.0)
        else:
            # DDPM Sampling
            samples = self.action_model.diffusion.p_sample_loop(sample_fn, 
                                                                    noise.shape, 
                                                                    noise, 
                                                                    clip_denoised=False,#False, try to set True 
                                                                    model_kwargs=model_kwargs,
                                                                    progress=False,
                                                                    device=cognition_features.device)
        if using_cfg:
            samples, _ = samples.chunk(2, dim=0)  # Remove null class samples
        normalized_actions = samples.cpu().numpy()

        # Un-normalize Actions
        action_norm_stats = self.get_action_stats(unnorm_key)
        mask = action_norm_stats.get("mask", np.ones_like(action_norm_stats["q01"], dtype=bool))
        action_high, action_low = np.array(action_norm_stats["q99"]), np.array(action_norm_stats["q01"])
        normalized_actions = np.clip(normalized_actions, -1, 1)
        normalized_actions[:, :, 6] = np.where(normalized_actions[:, :, 6] < 0.5, 0, 1) 
        actions = np.where(
            mask,
            0.5 * (normalized_actions + 1) * (action_high - action_low) + action_low,
            normalized_actions,
        )
        return actions, normalized_actions

    @staticmethod
    def _check_unnorm_key(norm_stats, unnorm_key):
        if unnorm_key is None:
            assert len(norm_stats) == 1, (
                f"Your model was trained on more than one dataset, "
                f"please pass a `unnorm_key` from the following options to choose the statistics "
                f"used for un-normalizing actions: {norm_stats.keys()}"
            )
            unnorm_key = next(iter(norm_stats.keys()))

        assert unnorm_key in norm_stats, (
            f"The `unnorm_key` you chose is not in the set of available dataset statistics, "
            f"please choose from: {norm_stats.keys()}"
        )
        return unnorm_key

    def get_action_dim(self, unnorm_key=None):
        """Dimensionality of the policy's action space."""
        unnorm_key = self._check_unnorm_key(self.norm_stats, unnorm_key)
        return len(self.norm_stats[unnorm_key]["action"]["q01"])

    def get_action_stats(self, unnorm_key=None):
        """Dimensionality of the policy's action space."""
        unnorm_key = self._check_unnorm_key(self.norm_stats, unnorm_key)
        return self.norm_stats[unnorm_key]["action"]
    
    @staticmethod
    def check_unexpected_keys(state_dict, model):
        """
        æ£€æŸ¥ state_dict æ˜¯å¦åŒ…å« unexpected_keysã€‚
        
        Args:
            state_dict (dict): éœ€è¦åŠ è½½çš„æ¨¡åž‹æƒé‡ã€‚
            model (torch.nn.Module): ç›®æ ‡æ¨¡åž‹ï¼ˆå¦‚ cogactï¼‰ã€‚
        
        Returns:
            bool: å¦‚æžœæ²¡æœ‰ unexpected_keysï¼Œåˆ™è¿”å›ž Trueï¼›å¦åˆ™è¿”å›ž False å¹¶æŠ¥é”™ã€‚
        """
        # èŽ·å–æ¨¡åž‹å·²æœ‰çš„å‚æ•°
        model_keys = set(model.state_dict().keys())

        # èŽ·å– state_dict é‡Œçš„å‚æ•°
        state_dict_keys = set(state_dict.keys())

        # è®¡ç®— unexpected_keys
        unexpected_keys = state_dict_keys - model_keys

        # å¦‚æžœå‘çŽ° unexpected_keysï¼ŒæŠ¥é”™æˆ–è€…è­¦å‘Š
        if unexpected_keys:
            print(f"âŒ Unexpected keys found in state_dict: {unexpected_keys}")
            return False  # å‘çŽ°ä¸åŒ¹é…çš„ keyï¼Œè¿”å›ž False
        
        print("âœ… No unexpected keys found.")
        return True  # æ‰€æœ‰ key éƒ½åŒ¹é…ï¼Œè¿”å›ž True
