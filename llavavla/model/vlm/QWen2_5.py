import torch
import torch.nn as nn
from typing import Optional, List, Dict, Any
from prismatic.models.backbones.llm.prompting import PromptBuilder
from functools import partial
from pathlib import Path
from typing import Callable, Dict, List, Optional, Type, Union

import torch
from PIL import Image
from torch.distributed.fsdp.wrap import _module_wrap_policy, _or_policy
from transformers.modeling_outputs import CausalLMOutputWithPast
from pathlib import Path
from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
from transformers.modeling_outputs import CausalLMOutputWithPast
from prismatic.models.vlms.base_vlm import VLM
from prismatic.overwatch import initialize_overwatch
from prismatic.util.nn_utils import FusedMLPProjector, LinearProjector, MLPProjector

# Initialize Overwatch =>> Wraps `logging.Logger`
overwatch = initialize_overwatch(__name__)

from prismatic.models.backbones.llm.prompting.base_prompter import PromptBuilder
from functools import partial
from torch.distributed.fsdp.wrap import transformer_auto_wrap_policy, size_based_auto_wrap_policy, _or_policy


from typing import Optional, Sequence, Type

import torch
from transformers import AutoModelForCausalLM
from torch.distributed.fsdp.wrap import (
    transformer_auto_wrap_policy,
    size_based_auto_wrap_policy,
    wrap,
)

# Registry =>> Support Qwen-2.5 Models (from HF Transformers)
# fmt: off
QWEN25_MODELS = {
    # === Pure Qwen2.5 (non-instruct/chat-tuned) Models ===
    "qwen25-0_5b-extra": {
        "llm_family": "qwen2.5", "llm_cls": AutoModelForCausalLM, "hf_hub_path": "Qwen/Qwen2.5-0.5B"
    },
    "qwen25-0_5b-pure": {
        "llm_family": "qwen2.5", "llm_cls": AutoModelForCausalLM, "hf_hub_path": "Qwen/Qwen2.5-0.5B"
    },
    "qwen25-1_5b-pure": {
        "llm_family": "qwen2.5", "llm_cls": AutoModelForCausalLM, "hf_hub_path": "Qwen/Qwen2.5-1.5B"
    },
    "qwen25-3b-pure": {
        "llm_family": "qwen2.5", "llm_cls": AutoModelForCausalLM, "hf_hub_path": "Qwen/Qwen2.5-3B"
    },
    "qwen25-7b-pure": {
        "llm_family": "qwen2.5", "llm_cls": AutoModelForCausalLM, "hf_hub_path": "Qwen/Qwen2.5-7B"
    },

}
# fmt: on


# add by jinhui
from llavavla.model.vlm.qwen_prompter import QwenPromptBuilder
from transformers.models.qwen2.modeling_qwen2 import Qwen2DecoderLayer
from torch.nn import Linear, Embedding

# ä» transformers å¯¼å…¥ Qwen2.5 ç›¸å…³æ¨¡å—
from transformers.models.qwen2.modeling_qwen2 import (
    Qwen2DecoderLayer,           # LLM Decoder Layer
)

from transformers.models.qwen2_5_vl.modeling_qwen2_5_vl import (
    Qwen2_5_VLVisionBlock,
    Qwen2_5_VLDecoderLayer,
)
from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor, AutoConfig


#@TODO emergency fix @Jinhui more readable and flexible way for VLM interface
class _QWen_VL_Interface(VLM): #TODO @Jinhui åæœŸä¸èƒ½å†å‘ PrismaticVLM å¯¹é½ï¼Œ æ€è€ƒæ›´åŠ flexibleåšæ³•ï¼Œ --ã€‹ æ¥å£classçš„å®ç°
    """
    è¿™æ˜¯å¯¹ Qwen2_5_VLForConditionalGeneration çš„ç®€å•å°è£…ï¼Œä½¿å…¶åœ¨æ¥å£å±‚é¢ä¸Šæ›´æ¥è¿‘ PrismaticVLMï¼Œ
    ä¾‹å¦‚èƒ½å¤Ÿè¿”å›ç±»ä¼¼ CausalLMOutputWithPast çš„ç»“æ„ï¼Œéœ€è¦ä¸€ä¸ª class æ¥åŒ…è£…æ˜¯å› ä¸º ä¸åŒçš„VLM æœ‰ä¸ä¸€æ ·çš„api, ä½†æ˜¯è¦ä¿è¯å¯¹å¤–çš„åŠŸèƒ½æ˜¯ä¸€è‡´çš„
    """

    def __init__(
        self,
        model_id: str,
        load_for_training: bool = True,
        enable_mixed_precision_training: bool = True, #@Jinhui Check
        **kwargs
    ):  


        super().__init__(
            "Qwen", #è¿™ä¸ªå…¶å®å¯ä»¥rm
            model_id,
            enable_mixed_precision_training=enable_mixed_precision_training,
        )
        # QWen åŸç”Ÿæ¨¡å‹
        if load_for_training: #TODO model -> vlm
            model = Qwen2_5_VLForConditionalGeneration.from_pretrained(model_id,  torch_dtype="auto", device_map="cpu") # åªèƒ½åˆ° cpu å…ˆ # è¯•è¯•auto --> FSDP è¿˜æ˜¯æŠ¥é”™äº†
        else:
            config = AutoConfig.from_pretrained(model_id)
            model = Qwen2_5_VLForConditionalGeneration(config)  # åªåˆå§‹åŒ–æ¨¡å‹ç»“æ„ï¼Œä¸åŠ è½½å‚æ•°

        processor = AutoProcessor.from_pretrained(model_id) #TODO check 
        processor.tokenizer.padding_side  = 'left' #TODO Check  Flash Attention version of Qwen2_5_VL. Make sure to  call `tokenizer.padding_side  = 'left'` before tokenizing the input. 


        self.model = model
        self.processor = processor
        # ä»…åšç¤ºä¾‹ï¼šç»™å‡ºä¸ PrismaticVLM æ¥å£å¯¹åº”çš„ä¸€äº›å ä½å±æ€§ # ä¸ºä»€ä¹ˆä¸ç»Ÿä¸€ï¼Ÿ
        # self.trainable_module_keys = ["model"] # TODO å°è¯•è®¾è®¡æ›´åŠ flexible  çš„diy æ–¹å¼
        # self.all_module_keys = ["model"] # ä¸åº”è¯¥ç”±è¿™é‡Œå‘å¤–ä¼ é€’åˆ°
        # è¿™é‡Œè¿˜å…¨éƒ¨éƒ½ä¸æ˜¯ FSDP
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        pixel_values: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        past_key_values: Optional[List[torch.FloatTensor]] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = True,  # éœ€è¦ hidden_states
        return_dict: Optional[bool] = True,
        **kwargs
    ) -> CausalLMOutputWithPast:
        """
        è°ƒç”¨ QWen2.5 çš„ forwardï¼Œè¾“å‡ºç±»ä¼¼ CausalLMOutputWithPast çš„ç»“æ„ï¼Œä¾› CogACT ä½¿ç”¨ã€‚
        """
        
        with torch.autocast("cuda", enabled=self.enable_mixed_precision_training, dtype=torch.float16):
            # @Jinhui TBD TODO 
            # pixel_values = pixel_values["pixel_values"]
            outputs = self.model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                pixel_values=pixel_values["pixel_values"],
                image_grid_thw =pixel_values["image_grid_thw"].reshape(-1, 3), #@Jinhui TODO mv to RLDSTransform
                labels=labels,
                use_cache=use_cache,
                output_attentions=output_attentions,
                output_hidden_states=output_hidden_states,
                return_dict=return_dict,
                past_key_values=past_key_values,
                inputs_embeds=inputs_embeds,
                **kwargs
            )
        
        # QWen2.5 é»˜è®¤è¿”å›çš„å¯èƒ½æ˜¯ QWenXXXModelOutputï¼›è¿™é‡Œç¤ºä¾‹å°†å®ƒåŒ…è£…æˆä¸€ä¸ª CausalLMOutputWithPast
        # ä»…åšç¤ºä¾‹ï¼šå¦‚æœ QWen2.5 è¿”å›çš„å­—æ®µåä¸åŒï¼Œä½ éœ€è¦åšå¯¹åº”å¤„ç†
        dummy_output = CausalLMOutputWithPast(
            loss=outputs.loss if hasattr(outputs, "loss") else None,
            logits=outputs.logits if hasattr(outputs, "logits") else None,
            past_key_values=outputs.past_key_values if hasattr(outputs, "past_key_values") else None,
            hidden_states=outputs.hidden_states if hasattr(outputs, "hidden_states") else None,
            attentions=outputs.attentions if hasattr(outputs, "attentions") else None,
        )
        return dummy_output

    def generate(
        self,
        input_ids: torch.LongTensor,
        attention_mask: torch.Tensor = None,
        pixel_values: torch.FloatTensor = None,
        max_new_tokens: int = 128,
        output_hidden_states: bool = True,
        return_dict_in_generate: bool = True,
        **kwargs
    ):
        """
        è®© Qwen2.5 å’Œ GPT ç±»ä¼¼åœ°è¿›è¡Œ generate ç”Ÿæˆã€‚
        æŸäº›å‚æ•°å¯èƒ½åœ¨ Qwen2.5 ä¸­ç”¨æ³•ä¸åŒï¼Œéœ€è¦ç»“åˆå®˜æ–¹æ–‡æ¡£è°ƒæ•´ã€‚
        """
        with torch.autocast("cuda", enabled=self.enable_mixed_precision_training, dtype=torch.float16):
            generation_output = self.model.generate(
                input_ids=input_ids,
                attention_mask=attention_mask,
                pixel_values=pixel_values,
                max_new_tokens=max_new_tokens,
                output_hidden_states=output_hidden_states,
                return_dict_in_generate=return_dict_in_generate,
                **kwargs
            )
        return generation_output

    @classmethod
    def from_pretrained(
        cls,
        model_id: str,
        enable_mixed_precision_training: bool = True,
        **kwargs
    ):
        """
        ç›´æ¥ çš„ from_pretrainedï¼Œç”¨äºç›´æ¥åŠ è½½ Qwen2.5ã€‚
        """
        return cls(model_id, enable_mixed_precision_training, **kwargs)

    ## Padding Methods
    def get_prompt_builder(self, system_prompt: Optional[str] = None) -> PromptBuilder:
        prompt_initializer: Type[PromptBuilder] = self.prompt_builder_fn
        return prompt_initializer(self.model_family, system_prompt=system_prompt)

    def freeze_backbones(self, stage: str) -> None:
        """ #@Jinhui 
        This function sets `requires_grad_` on each of the component modules explicitly, depending on stage.
        
        We support two separate stages --> "align" and "finetune".
            => "align" --> vision_backbone*, llm_backbone* are frozen; only the `projector` is trained.
            => "finetune" --> vision_backbone* is frozen; both `projector` and `llm_backbone` are trained.
        # @Jinhui TODO TODO ä¸ºäº†é«˜å†…èšï¼Œä¸è¦åœ¨å…¶ä»–åœ°æ–¹è®¾ç½®trainable æ¨¡å—è°ƒæ•´training ç­–ç•¥çš„ï¼Œè¦ç”¨é“¾é•¿äº†
        :param stage: Pretraining stage in < "align" | "finetune" | "full-finetune" | "vla-train" | "vla-full-train" >
        """
        if stage == "align": #@Jinhui TODO è¿™ä¸ªé¢„å®šä¹‰çš„ç­–ç•¥æŒºå¥½çš„ï¼Œä½†æ˜¯æˆ‘è¯¥é«˜å†…èšåˆ° VLA Class ä¸Šé¢
            self.vision_backbone.requires_grad_(False)
            self.llm_backbone.requires_grad_(False)
            # self.projector.requires_grad_(True)

            # Add to `self.trainable_module_keys`
            self.trainable_module_keys = ["projector"]

            # Update Trackers
            self.vision_backbone_requires_grad = False

            # Explicitly Log Frozen / Trainable Components
            overwatch.info(f"[Frozen]    ğŸ¥¶ =>> Vision Backbone `{self.vision_backbone.identifier}`", ctx_level=1)
            overwatch.info(f"[Frozen]    ğŸ¥¶ =>> LLM Backbone `{self.llm_backbone.identifier}`", ctx_level=1)
            overwatch.info(f"[TRAINABLE] ğŸ”¥ =>> Projector `{self.arch_specifier}`", ctx_level=1)

        elif stage in {"finetune", "vla-train"}:
            self.vision_backbone.requires_grad_(False)
            self.llm_backbone.requires_grad_(True)
            # self.projector.requires_grad_(True)

            # Add to `self.trainable_module_keys`
            self.trainable_module_keys = ["projector", "llm_backbone"]

            # Update Trackers
            self.vision_backbone_requires_grad = False

            # Explicitly Log Frozen / Unfrozen Components
            overwatch.info(f"[Frozen]    ğŸ¥¶ =>> Vision Backbone `{self.vision_backbone.identifier}`", ctx_level=1)
            overwatch.info(f"[TRAINABLE] ğŸ”¥ =>> LLM Backbone `{self.llm_backbone.identifier}`", ctx_level=1)
            overwatch.info(f"[TRAINABLE] ğŸ”¥ =>> Projector `{self.arch_specifier}`", ctx_level=1)

        elif stage in {"full-finetune", "vla-full-train"}:
            # self.vision_backbone.dtype = torch.float32 #ç›´æ¥ä¿®æ”¹dtypeå±æ€§å¯èƒ½ä¼šå¯¼è‡´é”™è¯¯
            for param in self.model.parameters():
                if param.dtype != torch.float32:
                    param.data = param.data.float()
            
            self.model.requires_grad_(True)
            # Add to `self.trainable_module_keys`
            self.trainable_module_keys = ["model"]
            # self.model.requires_grad_(False)
            # self.trainable_module_keys = []#["model"] 3

            # Update Trackers
            # self.vision_backbone_requires_grad = True

            # Explicitly Log Frozen / Unfrozen Components
            overwatch.info(f"[TRAINABLE] ğŸ”¥ =>> Backbone `{self.model.__class__.__name__}`", ctx_level=1)
            # overwatch.info(f"[TRAINABLE] ğŸ”¥ =>> LLM Backbone `{self.llm_backbone.__class__.__name__}`", ctx_level=1)
            # overwatch.info(f"[TRAINABLE] ğŸ”¥ =>> Projector `{self.arch_specifier}`", ctx_level=1)

        elif stage in {"last-layer-finetune", "vla-last-layer-train"}:
            self.model.requires_grad_(False)

            # Unfreeze final LLM layer
            for module in self.llm_backbone.last_layer_finetune_modules:
                module.requires_grad_(True)

            # Add to `self.trainable_module_keys`
            self.trainable_module_keys = ["llm_backbone"]

            # Update Trackers
            self.vision_backbone_requires_grad = False

            # Explicitly Log Frozen / Unfrozen Components
            # fmt: off
            overwatch.info(f"[Frozen]                    ğŸ¥¶   =>> Vision Backbone `{self.vision_backbone.identifier}`", ctx_level=1)  # noqa: E501
            overwatch.info(f"[Frozen, except last layer] ğŸ¥¶ğŸ”¥ =>> LLM Backbone `{self.llm_backbone.identifier}`", ctx_level=1)  # noqa: E501
            overwatch.info(f"[Frozen]                    ğŸ¥¶   =>> Projector `{self.arch_specifier}`", ctx_level=1)
            # fmt: on

        elif stage in {"vla-sandwich-train"}:
            # self.vision_backbone.dtype = torch.float32
            self.vision_backbone.to(torch.float32)
            self.vision_backbone.requires_grad_(True)
            self.projector.requires_grad_(True)
            # self.llm_backbone.requires_grad_(False)

            # Unfreeze final LLM layer
            for module in self.llm_backbone.last_layer_finetune_modules:
                module.requires_grad_(True)

            # Add to `self.trainable_module_keys`
            self.trainable_module_keys = ["vision_backbone", "projector", "llm_backbone"]

            # Update Trackers
            self.vision_backbone_requires_grad = True

            # Explicitly Log Frozen / Unfrozen Components
            # fmt: off
            overwatch.info(f"[TRAINABLE]                 ğŸ”¥   =>> Vision Backbone `{self.vision_backbone.identifier}`", ctx_level=1)  # noqa: E501
            overwatch.info(f"[Frozen, except last layer] ğŸ¥¶ğŸ”¥ =>> LLM Backbone `{self.llm_backbone.identifier}`", ctx_level=1)  # noqa: E501
            overwatch.info(f"[TRAINABLE]                 ğŸ”¥   =>> Projector `{self.arch_specifier}`", ctx_level=1)
            # fmt: on

        else:
            raise ValueError(f"Stage `{stage}` is not supported for LLaVa! Try < align | finetune >")

        overwatch.debug("##################################################")
        overwatch.debug("#####      Trainable Network Parameters:     #####")
        overwatch.debug("##################################################")
        for name, param in self.named_parameters():
            if param.requires_grad:
                overwatch.debug(name)

    def load_from_checkpoint(self, stage: str, run_dir: Path, pretrained_checkpoint: Optional[Path] = None) -> None:
        """Load weights from checkpoint (if required by the given stage)."""
        assert stage in {"align", "finetune", "full-finetune"}, f"Stage {stage} is not supported!"

        # If we're running a `no-align` architecture, we're good!
        if self.arch_specifier.startswith("no-align"):
            overwatch.info(
                f"PrismaticVLM with `{self.arch_specifier = }` does not require pretrained weights!", ctx_level=1
            )
            return

        # Otherwise, handle stage-specific logic!
        if stage == "align":
            overwatch.info("Stage `align` does not require pretrained weights =>> Starting Training", ctx_level=1)
            return

        # Otherwise, load from `pretrained_checkpoint` or match on `run_dir` (s/+stage-finetune/+stage-align/g)
        overwatch.info("Stage `finetune` requires `align` pretrained weights", ctx_level=1)

        # Config specifies path to a checkpoint to load
        if pretrained_checkpoint is not None:
            overwatch.info(f"Loading from Provided Checkpoint `{pretrained_checkpoint}`", ctx_level=1)
            model_state_dict = torch.load(pretrained_checkpoint)["model"]
            # self.projector.load_state_dict(model_state_dict["projector"])

            return

        # [Contract] If no `pretrained_checkpoint`, assume `align` lives in the run directory; string substitution!
        model, scale, _, seed = run_dir.name.split("+")
        align_dirs = [
            d
            for d in run_dir.parent.iterdir()
            if (d.name.startswith(f"{model}+{scale}") and d.name.endswith(f"+stage-align+{seed}"))
        ]
        assert len(align_dirs) == 1, "Multiple or No Valid Pretrained Directories Exist -- Double Check `runs`!"
        if (pretrained_checkpoint := (align_dirs[0] / "checkpoints" / "latest-checkpoint.pt")).exists():
            overwatch.info(f"Loading from Discovered Checkpoint `{pretrained_checkpoint}`", ctx_level=1)
            model_state_dict = torch.load(pretrained_checkpoint)["model"]
            # self.projector.load_state_dict(model_state_dict["projector"])
        else:
            raise ValueError(f"Could not find valid `align` checkpoint at {pretrained_checkpoint}!")


    def get_fsdp_wrapping_policy(self) -> Callable: #@Jinhui ç¡®å®ä¸éœ€è¦å®ç°å®ƒ
        """
        Return an FSDP wrapping policy that combines size-based and transformer-based auto-wrapping policies.
        """

        # 1ï¸âƒ£ Transformer-based Auto-Wrap Policy
        transformer_policy = partial(
            transformer_auto_wrap_policy,
            transformer_layer_cls={
                Qwen2_5_VLDecoderLayer,  # LLM è§£ç å±‚
                Qwen2_5_VLVisionBlock,   # è§†è§‰ Transformer Block
                nn.Linear,  # âœ… è¶…å¤§çš„ llm_head
            }
        )

        return transformer_policy #combined_policy @TODO Jinhui: æˆ–è®¸ QWen å†…éƒ¨æœ¬æ¥å°±æœ‰ fsdp

    @property
    def prompt_builder_fn(self) -> Type[PromptBuilder]:
        return QwenPromptBuilder #@Jinhui TODO è¿™ä¸ªå¯èƒ½æ˜¯ä¸ªå¥½çš„å®ç°
    
    @property
    def transformer_layer_cls(self) -> Type[torch.nn.Module]:
        return Qwen2DecoderLayer
    
    @property
    def half_precision_dtype(self) -> torch.dtype:
        return torch.bfloat16

    @property
    def last_layer_finetune_modules(self) -> Sequence[torch.nn.Module]:
        # TODO not sure that this works
        return (self.llm.model.embed_tokens, self.llm.model.layers[-1], self.llm.lm_head)

    
def get_qwen2_5_vl(model_id="playground/Pretrained_models/Qwen2.5-VL-7B-Instruct"):

    model = Qwen2_5_VLForConditionalGeneration.from_pretrained( # é‡Œé¢æœ‰å¥‡æ€ªçš„bug, æ¥è‡ªcookbooks
        model_id, torch_dtype="auto", device_map="auto"
    )
    processor = AutoProcessor.from_pretrained(model_id)

    return model, processor


if __name__ == "__main__":
    model_id = "./playground/Pretrained_models/Qwen2.5-VL-3B-Instruct"
    qwen_vl = _QWen_VL_Interface(model_id)
    pass