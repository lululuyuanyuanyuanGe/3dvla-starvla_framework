import torch
import torch.nn as nn
from typing import Optional, List, Dict, Any
from prismatic.models.backbones.llm.prompting import PromptBuilder
from functools import partial
from pathlib import Path
from typing import Callable, Dict, List, Optional, Type, Union

import torch
from PIL import Image
from torch.distributed.fsdp.wrap import _module_wrap_policy, _or_policy
from transformers.modeling_outputs import CausalLMOutputWithPast
from pathlib import Path
from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
from transformers.modeling_outputs import CausalLMOutputWithPast
from prismatic.models.vlms.base_vlm import VLM
from prismatic.overwatch import initialize_overwatch
from prismatic.util.nn_utils import FusedMLPProjector, LinearProjector, MLPProjector

# Initialize Overwatch =>> Wraps `logging.Logger`
overwatch = initialize_overwatch(__name__)

from prismatic.models.backbones.llm.prompting.base_prompter import PromptBuilder


from typing import Optional, Sequence, Type

import torch
from transformers import AutoModelForCausalLM
from torch.distributed.fsdp.wrap import (
    transformer_auto_wrap_policy,
    size_based_auto_wrap_policy,
    wrap,
)

# Registry =>> Support Qwen-2.5 Models (from HF Transformers)
# fmt: off
QWEN25_MODELS = {
    # === Pure Qwen2.5 (non-instruct/chat-tuned) Models ===
    "qwen25-0_5b-extra": {
        "llm_family": "qwen2.5", "llm_cls": AutoModelForCausalLM, "hf_hub_path": "Qwen/Qwen2.5-0.5B"
    },
    "qwen25-0_5b-pure": {
        "llm_family": "qwen2.5", "llm_cls": AutoModelForCausalLM, "hf_hub_path": "Qwen/Qwen2.5-0.5B"
    },
    "qwen25-1_5b-pure": {
        "llm_family": "qwen2.5", "llm_cls": AutoModelForCausalLM, "hf_hub_path": "Qwen/Qwen2.5-1.5B"
    },
    "qwen25-3b-pure": {
        "llm_family": "qwen2.5", "llm_cls": AutoModelForCausalLM, "hf_hub_path": "Qwen/Qwen2.5-3B"
    },
    "qwen25-7b-pure": {
        "llm_family": "qwen2.5", "llm_cls": AutoModelForCausalLM, "hf_hub_path": "Qwen/Qwen2.5-7B"
    },

}
# fmt: on


# add by jinhui
from llavavla.model.vlm.qwen_prompter import QwenPromptBuilder
from transformers.models.qwen2.modeling_qwen2 import Qwen2DecoderLayer
from torch.nn import Linear, Embedding

# ä» transformers å¯¼å…¥ Qwen2.5 ç›¸å…³æ¨¡å—
from transformers.models.qwen2.modeling_qwen2 import (
    Qwen2DecoderLayer,           # LLM Decoder Layer
    Qwen2MLP,                    # MLP Module in LLM
    Qwen2Attention,               # LLM Attention Layer
    Qwen2RMSNorm,                 # Normalization Layer
    Qwen2RotaryEmbedding          # Rotary Position Embedding
)

from transformers.models.qwen2_5_vl.modeling_qwen2_5_vl import (
    Qwen2_5_VisionTransformerPretrainedModel,
    Qwen2_5_VisionPatchEmbed,
    Qwen2_5_VLVisionBlock,
    Qwen2_5_VLSdpaAttention,
    Qwen2_5_VLMLP,
    Qwen2_5_VLPatchMerger,
    Qwen2_5_VisionRotaryEmbedding,
    Qwen2RMSNorm,
    Qwen2_5_VLModel,
    Qwen2_5_VLDecoderLayer,
    Qwen2_5_VLSdpaAttention,
    Qwen2MLP,
    Qwen2_5_VLRotaryEmbedding,
)

class _QWen_VL_Interface(VLM): #TODO @Jinhui åæœŸä¸èƒ½å†å‘ PrismaticVLM å¯¹é½ï¼Œ æ€è€ƒæ›´åŠ flexibleåšæ³•ï¼Œ --ã€‹ æ¥å£classçš„å®ç°
    """
    è¿™æ˜¯å¯¹ Qwen2_5_VLForConditionalGeneration çš„ç®€å•å°è£…ï¼Œä½¿å…¶åœ¨æ¥å£å±‚é¢ä¸Šæ›´æ¥è¿‘ PrismaticVLMï¼Œ
    ä¾‹å¦‚èƒ½å¤Ÿè¿”å›ç±»ä¼¼ CausalLMOutputWithPast çš„ç»“æ„ï¼Œå¹¶æ‹¥æœ‰ç±»ä¼¼ vision_backboneã€llm_backbone ç­‰å±æ€§ã€‚
    """

    def __init__(
        self,
        model_id: str,
        vision_backbone=None,
        llm_backbone=None,
        enable_mixed_precision_training: bool = True, #@Jinhui Check
        **kwargs
    ):  
        # QWen åŸç”Ÿæ¨¡å‹
        model = Qwen2_5_VLForConditionalGeneration.from_pretrained(model_id,  torch_dtype="auto", device_map="auto")
        # ä¼ªé€ å­æ¨¡å—å¼•ç”¨ï¼Œä»¥ä¾¿ CogACT é‡Œè¿˜èƒ½è®¿é—® æƒ³åŠæ³•æ‹¿åˆ°
        
        vision_backbone = model.visual
        # ä¸ºäº†å¯¹é½ self.llm_backbone # éœ€è¦è¿™æ ·å¹²çš„åŸå› æ˜¯ VLM_base å†™çš„ä¸å¥½ï¼Œåšäº†å¼ºåˆ¶å‡è®¾
        llm_backbone = model.model #
        processor = AutoProcessor.from_pretrained(model_id)
        llm_backbone.llm = llm_backbone.config
        llm_backbone.llm.generation_config  =  llm_backbone.generation_config

        super().__init__(
            "prismatic", #è¿™ä¸ªå…¶å®å¯ä»¥rm
            model_id,
            vision_backbone,
            llm_backbone,
            enable_mixed_precision_training=enable_mixed_precision_training,
        )
        # QWen åŸç”Ÿæ¨¡å‹
        self.model = model
        # å°†æ•´ä¸ªæ¨¡å‹è½¬æ¢ä¸ºæ‰€éœ€çš„ç²¾åº¦ç±»å‹ã€‚
        self.model.to(torch.float32)
        # ä¼ªé€ å­æ¨¡å—å¼•ç”¨ï¼Œä»¥ä¾¿ CogACT é‡Œè¿˜èƒ½è®¿é—® æƒ³åŠæ³•æ‹¿åˆ°
        # self.projector = self.model.lm_head #
        self.vision_backbone = self.model.visual
        # å¦‚æœéœ€è¦åœ¨ forward è¿‡ç¨‹ä¸­åšè‡ªåŠ¨æ··åˆç²¾åº¦
        self.enable_mixed_precision_training = enable_mixed_precision_training
        
        # å¤„ç†å›¾æ–‡è¾“å…¥
        self.processor = processor
        # ä»…åšç¤ºä¾‹ï¼šç»™å‡ºä¸ PrismaticVLM æ¥å£å¯¹åº”çš„ä¸€äº›å ä½å±æ€§
        self.trainable_module_keys = ["visual", "model", "lm_head"]
        self.all_module_keys = ["visual", "model", "lm_head"]
        
        # å¯¹é½ Keys
        self.arch_specifier = None #å…¶å®æ˜¯åœ¨  self.vision_backbone å†…éƒ¨

        self.llm_backbone.transformer_layer_cls = Qwen2DecoderLayer
  


    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        pixel_values: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        past_key_values: Optional[List[torch.FloatTensor]] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = True,  # éœ€è¦ hidden_states
        return_dict: Optional[bool] = True,
        **kwargs
    ) -> CausalLMOutputWithPast:
        """
        è°ƒç”¨ QWen2.5 çš„ forwardï¼Œè¾“å‡ºç±»ä¼¼ CausalLMOutputWithPast çš„ç»“æ„ï¼Œä¾› CogACT ä½¿ç”¨ã€‚
        """
        with torch.autocast("cuda", enabled=self.enable_mixed_precision_training, dtype=torch.float16):
            outputs = self.model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                pixel_values=pixel_values,
                labels=labels,
                use_cache=use_cache,
                output_attentions=output_attentions,
                output_hidden_states=output_hidden_states,
                return_dict=return_dict,
                past_key_values=past_key_values,
                inputs_embeds=inputs_embeds,
                **kwargs
            )
        
        # QWen2.5 é»˜è®¤è¿”å›çš„å¯èƒ½æ˜¯ QWenXXXModelOutputï¼›è¿™é‡Œç¤ºä¾‹å°†å®ƒåŒ…è£…æˆä¸€ä¸ª CausalLMOutputWithPast
        # ä»…åšç¤ºä¾‹ï¼šå¦‚æœ QWen2.5 è¿”å›çš„å­—æ®µåä¸åŒï¼Œä½ éœ€è¦åšå¯¹åº”å¤„ç†
        dummy_output = CausalLMOutputWithPast(
            loss=outputs.loss if hasattr(outputs, "loss") else None,
            logits=outputs.logits if hasattr(outputs, "logits") else None,
            past_key_values=outputs.past_key_values if hasattr(outputs, "past_key_values") else None,
            hidden_states=outputs.hidden_states if hasattr(outputs, "hidden_states") else None,
            attentions=outputs.attentions if hasattr(outputs, "attentions") else None,
        )
        return dummy_output

    def generate(
        self,
        input_ids: torch.LongTensor,
        attention_mask: torch.Tensor = None,
        pixel_values: torch.FloatTensor = None,
        max_new_tokens: int = 128,
        output_hidden_states: bool = True,
        return_dict_in_generate: bool = True,
        **kwargs
    ):
        """
        è®© Qwen2.5 å’Œ GPT ç±»ä¼¼åœ°è¿›è¡Œ generate ç”Ÿæˆã€‚
        æŸäº›å‚æ•°å¯èƒ½åœ¨ Qwen2.5 ä¸­ç”¨æ³•ä¸åŒï¼Œéœ€è¦ç»“åˆå®˜æ–¹æ–‡æ¡£è°ƒæ•´ã€‚
        """
        with torch.autocast("cuda", enabled=self.enable_mixed_precision_training, dtype=torch.float16):
            generation_output = self.model.generate(
                input_ids=input_ids,
                attention_mask=attention_mask,
                pixel_values=pixel_values,
                max_new_tokens=max_new_tokens,
                output_hidden_states=output_hidden_states,
                return_dict_in_generate=return_dict_in_generate,
                **kwargs
            )
        return generation_output

    def freeze_backbones(self, stage: str):
        """
        åŸæœ¬åœ¨ PrismaticVLM é‡Œå¯èƒ½ä¼šå†»ä½ vision_backbone æˆ– llm_backbone çš„æƒé‡ã€‚
        å¦‚æœ Qwen2.5 ä¹Ÿéœ€è¦åˆ†é˜¶æ®µå†»ç»“ï¼Œå¯ä»¥åœ¨è¿™é‡Œè‡ªå®šä¹‰é€»è¾‘ã€‚
        """
        # å¦‚æœä¸éœ€è¦å°±ç•™ç©º
        pass

    @classmethod
    def from_pretrained(
        cls,
        model_id: str,
        enable_mixed_precision_training: bool = True,
        **kwargs
    ):
        """
        ç±»ä¼¼ PrismaticVLM çš„ from_pretrainedï¼Œç”¨äºç›´æ¥åŠ è½½ Qwen2.5ã€‚
        """
        return cls(model_id, enable_mixed_precision_training, **kwargs)

    ## Padding Methods
    def get_prompt_builder(self, system_prompt: Optional[str] = None) -> PromptBuilder:
        prompt_initializer: Type[PromptBuilder] = self.prompt_builder_fn
        return prompt_initializer(self.model_family, system_prompt=system_prompt)

    def freeze_backbones(self, stage: str) -> None:
        """
        This function sets `requires_grad_` on each of the component modules explicitly, depending on stage.

        We support two separate stages --> "align" and "finetune".
            => "align" --> vision_backbone*, llm_backbone* are frozen; only the `projector` is trained.
            => "finetune" --> vision_backbone* is frozen; both `projector` and `llm_backbone` are trained.

        :param stage: Pretraining stage in < "align" | "finetune" | "full-finetune" | "vla-train" | "vla-full-train" >
        """
        if stage == "align":
            self.vision_backbone.requires_grad_(False)
            self.llm_backbone.requires_grad_(False)
            # self.projector.requires_grad_(True)

            # Add to `self.trainable_module_keys`
            self.trainable_module_keys = ["projector"]

            # Update Trackers
            self.vision_backbone_requires_grad = False

            # Explicitly Log Frozen / Trainable Components
            overwatch.info(f"[Frozen]    ğŸ¥¶ =>> Vision Backbone `{self.vision_backbone.identifier}`", ctx_level=1)
            overwatch.info(f"[Frozen]    ğŸ¥¶ =>> LLM Backbone `{self.llm_backbone.identifier}`", ctx_level=1)
            overwatch.info(f"[TRAINABLE] ğŸ”¥ =>> Projector `{self.arch_specifier}`", ctx_level=1)

        elif stage in {"finetune", "vla-train"}:
            self.vision_backbone.requires_grad_(False)
            self.llm_backbone.requires_grad_(True)
            # self.projector.requires_grad_(True)

            # Add to `self.trainable_module_keys`
            self.trainable_module_keys = ["projector", "llm_backbone"]

            # Update Trackers
            self.vision_backbone_requires_grad = False

            # Explicitly Log Frozen / Unfrozen Components
            overwatch.info(f"[Frozen]    ğŸ¥¶ =>> Vision Backbone `{self.vision_backbone.identifier}`", ctx_level=1)
            overwatch.info(f"[TRAINABLE] ğŸ”¥ =>> LLM Backbone `{self.llm_backbone.identifier}`", ctx_level=1)
            overwatch.info(f"[TRAINABLE] ğŸ”¥ =>> Projector `{self.arch_specifier}`", ctx_level=1)

        elif stage in {"full-finetune", "vla-full-train"}:
            # self.vision_backbone.dtype = torch.float32 #ç›´æ¥ä¿®æ”¹dtypeå±æ€§å¯èƒ½ä¼šå¯¼è‡´é”™è¯¯
            for param in self.vision_backbone.parameters():
                if param.dtype != torch.float32:
                    param.data = param.data.float()
            
            self.vision_backbone.requires_grad_(True)
            self.llm_backbone.requires_grad_(True)
        

            # Add to `self.trainable_module_keys`
            self.trainable_module_keys = ["vision_backbone", "projector", "llm_backbone"]

            # Update Trackers
            self.vision_backbone_requires_grad = True

            # Explicitly Log Frozen / Unfrozen Components
            overwatch.info(f"[TRAINABLE] ğŸ”¥ =>> Vision Backbone `{self.vision_backbone.__class__.__name__}`", ctx_level=1)
            overwatch.info(f"[TRAINABLE] ğŸ”¥ =>> LLM Backbone `{self.llm_backbone.__class__.__name__}`", ctx_level=1)
            overwatch.info(f"[TRAINABLE] ğŸ”¥ =>> Projector `{self.arch_specifier}`", ctx_level=1)

        elif stage in {"last-layer-finetune", "vla-last-layer-train"}:
            self.vision_backbone.requires_grad_(False)
            # self.projector.requires_grad_(False)         
            self.llm_backbone.requires_grad_(False)

            # Unfreeze final LLM layer
            for module in self.llm_backbone.last_layer_finetune_modules:
                module.requires_grad_(True)

            # Add to `self.trainable_module_keys`
            self.trainable_module_keys = ["llm_backbone"]

            # Update Trackers
            self.vision_backbone_requires_grad = False

            # Explicitly Log Frozen / Unfrozen Components
            # fmt: off
            overwatch.info(f"[Frozen]                    ğŸ¥¶   =>> Vision Backbone `{self.vision_backbone.identifier}`", ctx_level=1)  # noqa: E501
            overwatch.info(f"[Frozen, except last layer] ğŸ¥¶ğŸ”¥ =>> LLM Backbone `{self.llm_backbone.identifier}`", ctx_level=1)  # noqa: E501
            overwatch.info(f"[Frozen]                    ğŸ¥¶   =>> Projector `{self.arch_specifier}`", ctx_level=1)
            # fmt: on

        elif stage in {"vla-sandwich-train"}:
            # self.vision_backbone.dtype = torch.float32
            self.vision_backbone.to(torch.float32)
            self.vision_backbone.requires_grad_(True)
            # self.projector.requires_grad_(True)
            self.llm_backbone.requires_grad_(False)

            # Unfreeze final LLM layer
            for module in self.llm_backbone.last_layer_finetune_modules:
                module.requires_grad_(True)

            # Add to `self.trainable_module_keys`
            self.trainable_module_keys = ["vision_backbone", "projector", "llm_backbone"]

            # Update Trackers
            self.vision_backbone_requires_grad = True

            # Explicitly Log Frozen / Unfrozen Components
            # fmt: off
            overwatch.info(f"[TRAINABLE]                 ğŸ”¥   =>> Vision Backbone `{self.vision_backbone.identifier}`", ctx_level=1)  # noqa: E501
            overwatch.info(f"[Frozen, except last layer] ğŸ¥¶ğŸ”¥ =>> LLM Backbone `{self.llm_backbone.identifier}`", ctx_level=1)  # noqa: E501
            overwatch.info(f"[TRAINABLE]                 ğŸ”¥   =>> Projector `{self.arch_specifier}`", ctx_level=1)
            # fmt: on

        else:
            raise ValueError(f"Stage `{stage}` is not supported for LLaVa! Try < align | finetune >")

        overwatch.debug("##################################################")
        overwatch.debug("#####      Trainable Network Parameters:     #####")
        overwatch.debug("##################################################")
        for name, param in self.named_parameters():
            if param.requires_grad:
                overwatch.debug(name)

    def load_from_checkpoint(self, stage: str, run_dir: Path, pretrained_checkpoint: Optional[Path] = None) -> None:
        """Load weights from checkpoint (if required by the given stage)."""
        assert stage in {"align", "finetune", "full-finetune"}, f"Stage {stage} is not supported!"

        # If we're running a `no-align` architecture, we're good!
        if self.arch_specifier.startswith("no-align"):
            overwatch.info(
                f"PrismaticVLM with `{self.arch_specifier = }` does not require pretrained weights!", ctx_level=1
            )
            return

        # Otherwise, handle stage-specific logic!
        if stage == "align":
            overwatch.info("Stage `align` does not require pretrained weights =>> Starting Training", ctx_level=1)
            return

        # Otherwise, load from `pretrained_checkpoint` or match on `run_dir` (s/+stage-finetune/+stage-align/g)
        overwatch.info("Stage `finetune` requires `align` pretrained weights", ctx_level=1)

        # Config specifies path to a checkpoint to load
        if pretrained_checkpoint is not None:
            overwatch.info(f"Loading from Provided Checkpoint `{pretrained_checkpoint}`", ctx_level=1)
            model_state_dict = torch.load(pretrained_checkpoint)["model"]
            # self.projector.load_state_dict(model_state_dict["projector"])

            return

        # [Contract] If no `pretrained_checkpoint`, assume `align` lives in the run directory; string substitution!
        model, scale, _, seed = run_dir.name.split("+")
        align_dirs = [
            d
            for d in run_dir.parent.iterdir()
            if (d.name.startswith(f"{model}+{scale}") and d.name.endswith(f"+stage-align+{seed}"))
        ]
        assert len(align_dirs) == 1, "Multiple or No Valid Pretrained Directories Exist -- Double Check `runs`!"
        if (pretrained_checkpoint := (align_dirs[0] / "checkpoints" / "latest-checkpoint.pt")).exists():
            overwatch.info(f"Loading from Discovered Checkpoint `{pretrained_checkpoint}`", ctx_level=1)
            model_state_dict = torch.load(pretrained_checkpoint)["model"]
            # self.projector.load_state_dict(model_state_dict["projector"])
        else:
            raise ValueError(f"Could not find valid `align` checkpoint at {pretrained_checkpoint}!")

    def get_fsdp_wrapping_policy(self) -> Callable:
        """
        Return an FSDP wrapping policy that combines size-based and transformer-based auto-wrapping policies.
        """

        # 1ï¸âƒ£ Transformer-based Auto-Wrap Policy
        transformer_policy = partial(
            transformer_auto_wrap_policy,
            transformer_layer_cls={
                Qwen2_5_VLDecoderLayer,  # LLM è§£ç å±‚
                Qwen2_5_VLVisionBlock,   # è§†è§‰ Transformer Block
            }
        )

        # 2ï¸âƒ£ Size-based Auto-Wrap Policy (ç”¨äºè¶…å¤§å±‚)
        size_policy = partial(
            size_based_auto_wrap_policy,
            min_num_params=1e8  # 1 äº¿å‚æ•°ä»¥ä¸Šçš„å±‚è¿›è¡Œ FSDP åŒ…è£…
        )

        # 3ï¸âƒ£ ç»„åˆç­–ç•¥ï¼šä¼˜å…ˆåŒ¹é… Transformer å±‚ï¼Œå¦åˆ™åŸºäºå‚æ•°æ•°é‡åŒ…è£…
        def combined_policy(module, recurse, nonwrapped_numel):
            return transformer_policy(module, recurse, nonwrapped_numel) or size_policy(module, recurse, nonwrapped_numel)

        return combined_policy

    @property
    def prompt_builder_fn(self) -> Type[PromptBuilder]:
        return QwenPromptBuilder
    
    @property
    def transformer_layer_cls(self) -> Type[torch.nn.Module]:
        return Qwen2DecoderLayer
    
    @property
    def half_precision_dtype(self) -> torch.dtype:
        return torch.bfloat16

    @property
    def last_layer_finetune_modules(self) -> Sequence[torch.nn.Module]:
        # TODO not sure that this works
        return (self.llm.model.embed_tokens, self.llm.model.layers[-1], self.llm.lm_head)

    
def get_qwen2_5_vl(model_id="playground/Pretrained_models/Qwen2.5-VL-7B-Instruct"):

    model = Qwen2_5_VLForConditionalGeneration.from_pretrained( # é‡Œé¢æœ‰å¥‡æ€ªçš„bug, æ¥è‡ªcookbooks
        model_id, torch_dtype="auto", device_map="auto"
    )
    processor = AutoProcessor.from_pretrained(model_id)

    return model, processor


if __name__ == "__main__":
    model_id = "./playground/Pretrained_models/Qwen2.5-VL-3B-Instruct"
    qwen_vl = _QWen_VL_Interface(model_id)
    pass