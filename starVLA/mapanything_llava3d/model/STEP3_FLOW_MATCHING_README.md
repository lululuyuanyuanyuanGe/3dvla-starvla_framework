# æ­¥éª¤ 3: Flow Matchingä¸ Deep Fusion é›†æˆå®Œæˆ âœ…

**å®Œæˆæ—¥æœŸ**: 2024-12-30  
**æ ¸å¿ƒæ–‡ä»¶**: `modeling_flow_expert_dev.py`  
**å®ç°**: å®Œæ•´é‡å†™

---

## ğŸ“‹ å®ç°æ¦‚è¿°

æ­¥éª¤ 3 æˆåŠŸå°† **Flow Matching Action Expert** ä»åŸºäº Gemma çš„ Late Fusion æ¶æ„æ”¹é€ ä¸ºåŸºäº `LLaVA3DWithActionExpertModel` çš„ **Deep Fusion** æ¶æ„ï¼Œå®ç°äº†è§†è§‰-è¯­è¨€-åŠ¨ä½œçš„ç«¯åˆ°ç«¯æ·±åº¦äº¤äº’ã€‚

### æ ¸å¿ƒç‰¹æ€§

âœ… **å®Œå…¨åˆ é™¤ Gemma ä¾èµ–**
- ä¸å†ç»§æ‰¿ `GemmaPreTrainedModel`
- æ”¹ä¸ºçº¯ `nn.Module`
- ç½‘ç»œå‰å‘å§”æ‰˜ç»™ `LLaVA3DWithActionExpertModel`

âœ… **å®Œæ•´ä¿ç•™ Flow Matching æ•°å­¦**
- t ~ Uniform(0, 1) æ—¶é—´é‡‡æ ·
- x_t = t * noise + (1-t) * actions æ’å€¼
- u_t = noise - actions é€Ÿåº¦ç›®æ ‡
- Euler ODE solver ç§¯åˆ†

âœ… **çµæ´»çš„ Suffix ç»“æ„**
- æ”¯æŒ proprioceptive state (æœºå™¨äººçŠ¶æ€)
- Action tokens: æ¯ä¸ª action step ç‹¬ç«‹ embedding
- Time token: sinusoidal embedding + MLP

âœ… **Deep Fusion é›†æˆ**
- Prefix (vision+language) å’Œ Suffix (state+action+time) è”åˆå‰å‘
- æ¯å±‚éƒ½æœ‰ cross-attentionï¼ˆä¸å†æ˜¯ç®€å•çš„ global poolingï¼‰

---

## ğŸ—ï¸ æ¶æ„è®¾è®¡

### ç±»ç»“æ„

```python
class FlowMatchingActionExpert(nn.Module):
    """
    Flow Matching with LLaVA3D Deep Fusion
    
    ç»„ä»¶:
    - state_proj: [state_dim] â†’ [hidden_size] (optional)
    - action_in_proj: [action_dim] â†’ [hidden_size]
    - time_mlp_in/out: [hidden_size] â†’ [hidden_size]
    - action_out_proj: [hidden_size] â†’ [action_dim]
    - llava_with_expert: LLaVA3DWithActionExpertModel (å¤–éƒ¨)
    """
    
    def __init__(self, llava_with_expert_model, action_dim, action_horizon, ...)
    def _construct_suffix_embeddings(self, actions, time, state)
    def forward(self, prefix_embs, actions, time, state, ...)
    def compute_loss(self, prefix_embs, actions, state, ...)
    def sample_actions(self, prefix_embs, state, num_steps=10, ...)
```

### Suffix åºåˆ—ç»“æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ state_token â”‚ action_token_1, ..., H â”‚ time_token â”‚
â”‚ (optional)  â”‚                        â”‚            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    [B,1,H]         [B, H, H]           [B, 1, H]

Total length: (0 or 1) + H + 1 tokens
```

### Flow Matching æµç¨‹

**è®­ç»ƒ**:
```
Ground truth actions a âˆˆ R^{BÃ—HÃ—action_dim}
  â†“
Sample t ~ U(0,1), Îµ ~ N(0,I)
  â†“
Construct x_t = tÂ·Îµ + (1-t)Â·a
  â†“
Target velocity u_t = Îµ - a
  â†“
Predict v_t = model(prefix_embs, x_t, t, state)
  â†“
Loss = MSE(v_t, u_t)
```

### åŠ¨ä½œ Horizon ä¸æ•°æ®å¯¹é½ï¼ˆä¸ openpi ä¿æŒä¸€è‡´ï¼‰

- åœ¨åŸç”Ÿ openpi è®­ç»ƒä¸­ï¼Œ`actions` çš„å½¢çŠ¶ç”±é…ç½®ä¸­çš„ `config.model.action_horizon` å†³å®šï¼š
  - DataLoader é€šè¿‡ `create_torch_dataset(..., action_horizon=config.model.action_horizon, ...)` è°ƒç”¨ `LeRobotDataset`
  - `LeRobotDataset` ä½¿ç”¨ `delta_timestamps` å’Œ `fps` åœ¨æ—¶é—´è½´ä¸Šæ„é€ é•¿åº¦ä¸º `H = action_horizon` çš„åŠ¨ä½œåºåˆ—
  - æµ‹è¯•ä¸­æ˜¾å¼æ–­è¨€ï¼š`actions.shape == (batch_size, action_horizon, action_dim)`
- åœ¨æœ¬é¡¹ç›®çš„ Deep Fusion é›†æˆä¸­ï¼Œæˆ‘ä»¬å¯¹ Libero èµ°åŒæ ·çš„è·¯å¾„ï¼š
  - å¯¹äº `repo_id="physical-intelligence/libero"` ä¸”è®¾ç½®äº† `LIBERO_LOCAL_ROOT` çš„æƒ…å†µï¼Œ
    `create_torch_dataset` ä¼šç›´æ¥è°ƒç”¨æœ¬åœ° `LeRobotDataset`ï¼Œä¼ å…¥ç›¸åŒçš„ `action_horizon` å’Œ `delta_timestamps`
  - è¿™æ ·ä» DataLoader è¾“å‡ºçš„ `actions` ä¹Ÿæ»¡è¶³ï¼š
    `actions.shape == (batch_size, config.model.action_horizon, config.model.action_dim)`
- FlowMatchingActionExpert å§‹ç»ˆå‡å®šï¼š
  - è¾“å…¥ `actions` çš„æ—¶é—´ç»´ `H` ä¸é…ç½®çš„ `action_horizon` ä¸€è‡´
  - Flow Matching ä¸­çš„ `x_t`, `u_t`, `pred_velocity`ã€ä»¥åŠ suffix åºåˆ—é•¿åº¦éƒ½å›´ç»•è¿™ä¸ªç»Ÿä¸€çš„ `H` å±•å¼€
  - ä»è€Œé¿å… `pred_velocity` å’Œ `target_velocity` åœ¨æ—¶é—´ç»´ä¸Šå‡ºç°å¹¿æ’­å¼çš„ shape mismatch

**æ¨ç†**:
```
Start: x_1 ~ N(0, I) (pure noise)
  â†“
For t from 1.0 to 0.0 (step=-1/num_steps):
  v_t = model(prefix_embs, x_t, t, state)
  x_t = x_t + v_t * dt
  â†“
End: x_0 (clean actions)
```

---

## ğŸ”§ ä½¿ç”¨æ–¹æ³•

### åˆå§‹åŒ–

```python
from modeling_llava3d_v2_dev import LLaVA3DForCausalLMV2, LLaVA3DWithActionExpertModel
from modeling_flow_expert_dev import FlowMatchingActionExpert

# 1. åŠ è½½ base LLaVA3D
base_llava = LLaVA3DForCausalLMV2.from_pretrained("path/to/llava3d")

# 2. åˆ›å»º Deep Fusion æ¨¡å‹
llava_with_expert = LLaVA3DWithActionExpertModel(base_llava)

# 3. åˆ›å»º Flow Matching Expert
flow_expert = FlowMatchingActionExpert(
    llava_with_expert_model=llava_with_expert,
    action_dim=7,              # æœºå™¨äººè‡ªç”±åº¦
    action_horizon=10,         # é¢„æµ‹æ­¥æ•°
    state_dim=14,              # å…³èŠ‚è§’åº¦+é€Ÿåº¦
    use_state=True,            # ä½¿ç”¨ proprioceptive state
)
```

### è®­ç»ƒ

```python
# å‡†å¤‡è¾“å…¥
prefix_embs = get_image_text_embeddings(...)  # [B, L_p, H]
actions = get_ground_truth_actions(...)       # [B, 10, 7]
state = get_robot_state(...)                  # [B, 14]

# è®¡ç®— Flow Matching loss
loss = flow_expert.compute_loss(
    prefix_embs=prefix_embs,
    actions=actions,
    state=state,
)

# åå‘ä¼ æ’­
loss.backward()
optimizer.step()
```

### æ¨ç†

```python
# é¢„æµ‹åŠ¨ä½œ
with torch.no_grad():
    predicted_actions = flow_expert.sample_actions(
        prefix_embs=prefix_embs,
        state=state,
        num_steps=20,  # æ›´å¤šæ­¥æ•° = æ›´ç²¾ç¡®
    )  # [B, 10, 7]

# æ‰§è¡ŒåŠ¨ä½œ
robot.execute(predicted_actions[0])  # [10, 7]
```

---

## ğŸ“Š ä¸åŸå§‹å®ç°çš„å¯¹æ¯”

| ç»´åº¦ | Late Fusion (åŸå§‹) | Deep Fusion (æœ¬å®ç°) |
|------|-------------------|----------------------|
| **ç½‘ç»œ** | ç‹¬ç«‹ Gemma Expert | LLaVA3D å…±äº« Transformer |
| **ä¸Šä¸‹æ–‡** | Global pooling (mean) | âœ… æ¯å±‚ cross-attention |
| **å‚æ•°é‡** | +3B (Gemma) | âœ… 0 (å¤ç”¨ LLaVA3D) |
| **è§†è§‰äº¤äº’** | ä»…æœ€åä¸€å±‚ | âœ… æ¯å±‚éƒ½äº¤äº’ |
| **State æ”¯æŒ** | æ—  | âœ… å¯é€‰ state token |
| **è¡¨è¾¾èƒ½åŠ›** | ä¸­ (æµ…å±‚èåˆ) | âœ… å¼º (æ·±å±‚èåˆ) |

### æ•ˆæœæå‡ï¼ˆç†è®ºé¢„æœŸï¼‰

- **æ›´å¼ºçš„è§†è§‰ç†è§£**: Suffix åœ¨æ¯å±‚éƒ½èƒ½ attend to prefix
- **æ›´ç²¾ç¡®çš„åŠ¨ä½œé¢„æµ‹**: æ·±å±‚ Transformer æ›¿ä»£æµ…å±‚ MLP
- **æ›´å°‘çš„å‚æ•°**: å¤ç”¨ LLaVA3Dï¼Œæ— éœ€é¢å¤– Gemma Expert
- **æ›´å¥½çš„æ³›åŒ–**: è§†è§‰-è¯­è¨€-åŠ¨ä½œç«¯åˆ°ç«¯è®­ç»ƒ

---

## ğŸ” ä»£ç è¯¦è§£

### æ ¸å¿ƒæ–¹æ³• 1: `_construct_suffix_embeddings`

**åŠŸèƒ½**: å°† state, actions, time è½¬æ¢ä¸º suffix token åºåˆ—

```python
def _construct_suffix_embeddings(self, actions, time, state):
    """
    è¾“å…¥:
    - actions: [B, H, action_dim] noisy or clean actions
    - time: [B] time values in [0, 1]
    - state: [B, state_dim] proprioceptive state (optional)
    
    è¾“å‡º:
    - suffix_embs: [B, L_s, hidden_size]
      where L_s = (0 or 1) + H + 1
    """
    suffix_tokens = []
    
    # 1. State token (optional)
    if self.use_state and state is not None:
        state_token = self.state_proj(state).unsqueeze(1)  # [B, 1, H]
        suffix_tokens.append(state_token)
    
    # 2. Action tokens
    action_tokens = self.action_in_proj(actions)  # [B, H, hidden_size]
    suffix_tokens.append(action_tokens)
    
    # 3. Time token
    time_embed = create_sinusoidal_pos_embedding(time, self.hidden_size, ...)
    time_embed = self.time_mlp_in(time_embed)
    time_embed = F.silu(time_embed)
    time_embed = self.time_mlp_out(time_embed)
    time_token = time_embed.unsqueeze(1)  # [B, 1, hidden_size]
    suffix_tokens.append(time_token)
    
    # Concatenate
    suffix_embs = torch.cat(suffix_tokens, dim=1)
    return suffix_embs
```

### æ ¸å¿ƒæ–¹æ³• 2: `forward` (Deep Fusion)

**åŠŸèƒ½**: é€šè¿‡ LLaVA3D Deep Fusion é¢„æµ‹ velocity

```python
def forward(self, prefix_embs, actions, time, state, ...):
    """
    è¾“å…¥:
    - prefix_embs: [B, L_p, H] vision + language
    - actions: [B, H, action_dim] noisy actions x_t
    - time: [B] current time t
    - state: [B, state_dim] robot state
    
    è¾“å‡º:
    - pred_velocity: [B, H, action_dim]
    """
    # Step 1: Construct suffix
    suffix_embs = self._construct_suffix_embeddings(actions, time, state)
    
    # Step 2: Deep Fusion forward
    outputs, _ = self.llava_with_expert(
        inputs_embeds=[prefix_embs, suffix_embs],  # Mode 3: joint
        attention_mask=attention_mask,
        position_ids=position_ids,
    )
    
    prefix_output, suffix_output = outputs
    # æ³¨æ„: prefix å’Œ suffix åœ¨æ¯å±‚éƒ½äº’ç›¸æ„ŸçŸ¥äº†ï¼
    
    # Step 3: Extract action tokens
    if self.use_state:
        # Skip state_token and time_token
        action_hidden = suffix_output[:, 1:1+self.action_horizon, :]
    else:
        # Skip time_token
        action_hidden = suffix_output[:, :self.action_horizon, :]
    
    # Step 4: Project to velocity
    pred_velocity = self.action_out_proj(action_hidden)
    
    return pred_velocity
```

### æ ¸å¿ƒæ–¹æ³• 3: `compute_loss` (Flow Matching Training)

**åŠŸèƒ½**: å®ç° Flow Matching è®­ç»ƒç®—æ³•

```python
def compute_loss(self, prefix_embs, actions, state, ...):
    """
    Flow Matching Loss:
    1. Sample t ~ U(0,1) and noise ~ N(0,I)
    2. Construct x_t = t*noise + (1-t)*actions
    3. Target u_t = noise - actions
    4. Predict v_t = forward(prefix_embs, x_t, t, state)
    5. Loss = MSE(v_t, u_t)
    """
    batch_size = actions.shape[0]
    device = actions.device
    
    # Sample time and noise
    t = torch.rand((batch_size,), device=device)
    noise = torch.randn_like(actions)
    
    # Construct noisy actions
    t_exp = t.view(batch_size, 1, 1)
    x_t = t_exp * noise + (1 - t_exp) * actions
    
    # Target velocity
    target_velocity = noise - actions
    
    # Predict velocity
    pred_velocity = self.forward(prefix_embs, x_t, t, state, ...)
    
    # MSE loss
    loss = F.mse_loss(pred_velocity, target_velocity)
    
    return loss
```

### æ ¸å¿ƒæ–¹æ³• 4: `sample_actions` (Euler ODE Sampling)

**åŠŸèƒ½**: ä½¿ç”¨ Euler ODE solver ä»å™ªå£°ç”Ÿæˆ clean actions

```python
@torch.no_grad()
def sample_actions(self, prefix_embs, state, num_steps=10, ...):
    """
    Euler ODE Solver:
    - Start: x_1 ~ N(0, I)
    - Loop: x_t = x_t + v_t * dt (t: 1 â†’ 0)
    - End: x_0 (clean actions)
    """
    batch_size = prefix_embs.shape[0]
    device = prefix_embs.device
    
    # Initialize with noise
    action_shape = (batch_size, self.action_horizon, self.action_dim)
    x_t = torch.randn(action_shape, device=device)
    
    # Time step
    dt = -1.0 / num_steps  # Negative (backward in time)
    
    # Euler integration
    for step in range(num_steps):
        t_curr = 1.0 + step * dt  # 1.0, 0.9, 0.8, ..., 0.1
        t_tensor = torch.full((batch_size,), t_curr, device=device)
        
        # Predict velocity
        v_t = self.forward(prefix_embs, x_t, t_tensor, state, ...)
        
        # Euler step
        x_t = x_t + v_t * dt
    
    return x_t  # x_0
```

---

## ğŸ“ˆ è¿›åº¦æ›´æ–°

```
æ­¥éª¤ 0: åˆ›å»º dev å‰¯æœ¬         âœ… å®Œæˆ
æ­¥éª¤ 1: æœ€å°ç‰ˆæœ¬æ¡†æ¶          âœ… å®Œæˆ  
æ­¥éª¤ 2: åŒæµè”åˆæ³¨æ„åŠ›        âœ… å®Œæˆ
æ­¥éª¤ 3: Flow Matching é›†æˆ   âœ… å®Œæˆ â­ (æœ¬æ¬¡)
æ­¥éª¤ 4: Wrapper é›†æˆ         â³ å¾…å®ç°
```

**æ•´ä½“å®Œæˆåº¦ï¼š~75%**

---

## ğŸš€ ä¸‹ä¸€æ­¥ï¼ˆæ­¥éª¤ 4ï¼‰

æ­¥éª¤ 3 å·²ç»å®Œæˆäº† Flow Matching ç®—æ³•å±‚ï¼Œä¸‹ä¸€æ­¥éœ€è¦åœ¨é¡¶å±‚ wrapper ä¸­é›†æˆï¼š

### æ­¥éª¤ 4: æ”¹é€  `MapAnythingLlava3DForConditionalGeneration`

**ä»»åŠ¡æ¸…å•**:
1. **åˆå§‹åŒ– FlowMatchingActionExpert**
   ```python
   self.action_expert = FlowMatchingActionExpert(
       llava_with_expert_model=self.language_model_with_expert,
       action_dim=config.action_dim,
       action_horizon=config.action_horizon,
       ...
   )
   ```

2. **æ„é€  prefix_embs** (åœ¨ `forward`)
   ```python
   # Get image + geometric features
   image_features = self.get_image_features(pixel_values, intrinsic)
   
   # Get text embeddings
   text_embeds = self.get_input_embeddings()(input_ids)
   
   # Inject image features at <image> token positions
   prefix_embs = inject_image_to_text(text_embeds, image_features, image_mask)
   ```

3. **è®­ç»ƒè·¯å¾„** (åœ¨ `forward` with actions)
   ```python
   if actions is not None and self.action_expert is not None:
       action_loss = self.action_expert.compute_loss(
           prefix_embs=prefix_embs,
           actions=actions,
           state=state,
       )
       return MapAnythingLlava3DOutput(loss=action_loss, ...)
   ```

4. **æ¨ç†è·¯å¾„** (åœ¨ `predict_action`)
   ```python
   # Construct prefix
   prefix_embs = ...
   
   # Sample actions
   predicted_actions = self.action_expert.sample_actions(
       prefix_embs=prefix_embs,
       state=state,
       num_steps=20,
   )
   
   return predicted_actions
   ```

---

## ğŸ“š å‚è€ƒæ–‡æ¡£

- **å®ç°æ–¹æ¡ˆ**: `llava3d_deep_fusion_plan.md`ï¼ˆå·²æ›´æ–°æ­¥éª¤ 3 çŠ¶æ€ï¼‰
- **æ­¥éª¤ 2**: `STEP2_DEEP_FUSION_README.md`
- **PI0 å‚è€ƒ**: `mapAnythingLlava3dPi0.5/openpi/models_pytorch/`

---

## âœ¨ è´¡çŒ®è€…

**å®ç°è€…**: AI Assistant  
**å®¡æ ¸è€…**: å¾…å®š  
**æ—¥æœŸ**: 2024-12-30

---

## ğŸ“ æ›´æ–°æ—¥å¿—

### 2024-12-30
- âœ… å®Œå…¨åˆ é™¤ Gemma ä¾èµ–
- âœ… å®ç° `_construct_suffix_embeddings` (state + actions + time)
- âœ… å®ç° `forward` (Deep Fusion é›†æˆ)
- âœ… å®ç° `compute_loss` (Flow Matching è®­ç»ƒ)
- âœ… å®ç° `sample_actions` (Euler ODE é‡‡æ ·)
- âœ… æ·»åŠ  proprioceptive state æ”¯æŒ
- âœ… å®Œæ•´çš„æ–‡æ¡£å’Œæ³¨é‡Š

---

**çŠ¶æ€**: âœ… æ­¥éª¤ 3 å®Œæˆ  
**ä¸‹ä¸€æ­¥**: ğŸš§ æ­¥éª¤ 4 - é›†æˆåˆ° MapAnythingLlava3DForConditionalGeneration
