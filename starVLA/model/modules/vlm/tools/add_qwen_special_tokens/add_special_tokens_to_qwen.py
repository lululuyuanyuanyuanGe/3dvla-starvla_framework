# Copyright 2025 starVLA community. All rights reserved.
# Licensed under the MIT License, Version 1.0 (the "License"); 
# Implemented by [Jinhui YE / HKUST University] in [2025].


import argparse
import json
import os
from typing import List, Dict, Tuple

import torch
import torch.nn as nn
from transformers import AutoTokenizer, Qwen2_5_VLForConditionalGeneration, AutoProcessor
from transformers import Qwen3VLForConditionalGeneration, AutoProcessor

def add_new_tokens(
    model,
    tokenizer,
    new_tokens: List[str],
    init_strategy: str = "avg",
    as_special: bool = True,
) -> Tuple[Dict[str, int], int, int, int]:
    """
    å‘æ¨¡å‹ä¸ tokenizer ä¸­æ·»åŠ æ–°çš„ tokensï¼ˆè‹¥ä¸å­˜åœ¨ï¼‰ã€‚
    init_strategy: avg / normal / zero
    è¿”å›:
      - mapping: æ‰€æœ‰ç›®æ ‡ tokens çš„ token_id æ˜ å°„
      - added_now: æœ¬æ¬¡å®é™…æ–°å¢åˆ° tokenizer çš„æ•°é‡
      - action_token_start_idx: æ–°å¢ embedding èµ·å§‹ä¸‹æ ‡ï¼ˆæŒ‰æ¨¡å‹æ—§ embedding å¤§å°è®¡ç®—ï¼‰
      - action_token_end_idx: æ–°å¢ embedding ç»“æŸä¸‹æ ‡ï¼ˆè‹¥æ— æ–°å¢ï¼Œåˆ™ä¸º start_idx-1ï¼‰
    è¯´æ˜:
      - tokenizer.vocab_size ä¸ºåŸºç¡€è¯è¡¨å¤§å°ï¼ˆä¸å«å·²æ·»åŠ çš„ special/added tokensï¼‰
      - len(tokenizer) ä¸ºæ€»è¯è¡¨å¤§å°ï¼ˆå« added/special tokensï¼‰
      - æ¨¡å‹çš„æ—§ embedding å¤§å°ä»¥ model.get_input_embeddings().weight.shape[0] ä¸ºå‡†
    """
    # 1) è®¡ç®—éœ€è¦æ–°å¢çš„ tokensï¼ˆç›¸å¯¹ tokenizer ç°æœ‰ vocabï¼‰
    vocab = tokenizer.get_vocab()  # å«åŸæœ‰çš„ç‰¹æ®Š tokens
    to_add_tokens = [t for t in new_tokens if t not in vocab]

    # 2) è®°å½•æ¨¡å‹å½“å‰çš„ embedding å°ºå¯¸ï¼ˆåŸºç¡€å¤§å°ï¼‰
    old_embed = model.get_input_embeddings()
    old_embed_size = old_embed.weight.shape[0] # æ˜¯åŒ…æ‹¬QWen è‡ªç•™çš„ token çš„

    # 3) å¦‚æœ‰éœ€è¦ï¼Œå…ˆæŠŠ tokens åŠ åˆ° tokenizer
    added_now = 0
    if to_add_tokens:
        if as_special:
            added_now = tokenizer.add_special_tokens({"additional_special_tokens": to_add_tokens})
        else:
            added_now = tokenizer.add_tokens(to_add_tokens)

    # 4) ç›®æ ‡æ€»å¤§å°ï¼ˆtokenizer æ€»å¤§å°ï¼ŒåŸºç¡€ + æ‰€æœ‰å·²æ·»åŠ ï¼‰
    # target_size = len(tokenizer) # æ€»è¯è¡¨ --> æ˜¯å¦è¦ä¿ç•™ä¹‹å‰é¢„ç•™çš„ ç©ºtokenï¼Ÿ
    target_size = old_embed_size + added_now
    # 5) è‹¥ tokenizer æ€»å¤§å°å¤§äºæ¨¡å‹ embedding å¤§å°ï¼Œåˆ™éœ€è¦ resize å¹¶åˆå§‹åŒ–æ–°å¢è¡Œ
    action_token_start_idx = old_embed_size # è¿™é‡Œæ˜¯ä¸ä¿ç•™æ–¹æ¡ˆ
    action_token_end_idx = old_embed_size - 1  # é»˜è®¤â€œæ— æ–°å¢â€
    if target_size > old_embed_size:
        model.resize_token_embeddings(target_size) # è¿™é‡Œä¸è¯¥resize target, ä¼šå’Œ tokenizer ä¸åŒ¹é…
        new_embed = model.get_input_embeddings()
        with torch.no_grad():
            if init_strategy == "avg":
                ref_vec = old_embed.weight.mean(dim=0, keepdim=True)
                for idx in range(old_embed_size, target_size):
                    new_embed.weight[idx].copy_(ref_vec[0])
            elif init_strategy == "zero":
                for idx in range(old_embed_size, target_size):
                    new_embed.weight[idx].zero_()
            elif init_strategy == "normal":
                for idx in range(old_embed_size, target_size):
                    nn.init.normal_(new_embed.weight[idx], mean=0.0, std=0.02)
            else:
                raise ValueError(f"æœªçŸ¥ init_strategy: {init_strategy}")

        action_token_end_idx = target_size - 1

    # 6) æ„é€ æ˜ å°„ï¼ˆè¿”å›è¯·æ±‚å…³å¿ƒçš„ tokens çš„ idï¼‰
    mapping = {t: tokenizer.convert_tokens_to_ids(t) for t in new_tokens}
    return mapping, added_now, action_token_start_idx, action_token_end_idx

def save_bundle(model, tokenizer, mapping: Dict[str, int], save_dir: str, processor_src: str | None = None, padding_side: str | None = None):
    os.makedirs(save_dir, exist_ok=True)
    model.save_pretrained(save_dir)
    tokenizer.save_pretrained(save_dir)
    with open(os.path.join(save_dir, "added_custom_token_id_map.json"), "w", encoding="utf-8") as f:
        json.dump(mapping, f, ensure_ascii=False, indent=2)
    print(f"[OK] å·²ä¿å­˜åˆ°: {save_dir}")

    # é¢å¤–ä¿å­˜ AutoProcessorï¼ˆç”Ÿæˆ preprocessor_config.jsonï¼‰ï¼Œä»¥ä¾¿ AutoProcessor.from_pretrained(...) åŠ è½½
    try:
        src = processor_src or save_dir
        processor = AutoProcessor.from_pretrained(src, trust_remote_code=True)
        # åŒæ­¥ processor.tokenizer 
        processor.tokenizer = tokenizer
        processor.save_pretrained(save_dir)
        print(f"[OK] AutoProcessor å·²ä¿å­˜åˆ°: {save_dir}")
    except Exception as e:
        print(f"[WARN] ä¿å­˜ AutoProcessor å¤±è´¥: {e}")

def reload_and_check(save_dir: str, tokens: List[str]) -> bool:
    tok = AutoTokenizer.from_pretrained(save_dir, trust_remote_code=True)
    vocab = tok.get_vocab()
    missing = [t for t in tokens if t not in vocab]
    if missing:
        print(f"[WARN] é‡æ–°åŠ è½½åä»ç¼ºå¤±: {missing}")
        return False
    print("[OK] é‡æ–°åŠ è½½æ£€æŸ¥é€šè¿‡ï¼Œæ‰€æœ‰ token å‡å­˜åœ¨ã€‚")
    return True

def parse_tokens(args) -> List[str]:
    tokens: List[str] = []
    if args.tokens:
        tokens.extend([t.strip() for t in args.tokens.split(",") if t.strip()])
    if args.tokens_file:
        with open(args.tokens_file, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if line:
                    tokens.append(line)
    # å»é‡ä¿æŒé¡ºåº
    seen = set()
    ordered = []
    for t in tokens:
        if t not in seen:
            seen.add(t)
            ordered.append(t)
    return ordered

def main():
    parser = argparse.ArgumentParser(
        description="ä¸º Qwen2.5-VL æ¨¡å‹æ·»åŠ ç‰¹æ®Š tokens å¹¶ä¿å­˜åˆ°æœ¬åœ°ã€‚"
    )
    parser.add_argument("--model-id", default="Qwen/Qwen2.5-VL-3B-Instruct", help="HF Hub æ¨¡å‹æˆ–æœ¬åœ°è·¯å¾„")
    parser.add_argument("--save-dir", required=True, help="ä¿å­˜ç›®å½•")
    parser.add_argument("--tokens", default="", help="é€—å·åˆ†éš” tokensï¼Œä¾‹å¦‚: <loc_x>,<loc_y>")
    parser.add_argument("--tokens-file", help="åŒ…å«å¾…æ·»åŠ  token çš„æ–‡æœ¬æ–‡ä»¶ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰")
    parser.add_argument("--init-strategy", default="avg", choices=["avg", "normal", "zero"], help="æ–°å¢ embedding åˆå§‹åŒ–ç­–ç•¥")
    parser.add_argument("--as-special", action="store_true", help="æ˜¯å¦ä½œä¸º special tokens æ·»åŠ ")
    parser.add_argument("--no-as-special", dest="as_special", action="store_false")
    parser.set_defaults(as_special=True)
    parser.add_argument("--padding-side", default="left", choices=["left", "right"])
    parser.add_argument("--device", default="cuda", help="cuda / cpu / mps / auto")
    args = parser.parse_args()

    tokens = parse_tokens(args)
    if not tokens:
        print("æœªæä¾›ä»»ä½• tokenï¼Œå¯ä½¿ç”¨ --tokens æˆ– --tokens-file")
        return

    print(f"[INFO] å¾…å¤„ç† tokens: {tokens}")

    print(f"[INFO] åŠ è½½æ¨¡å‹: {args.model_id}")
    tokenizer = AutoTokenizer.from_pretrained(args.model_id, trust_remote_code=True)
    tokenizer.padding_side = args.padding_side
    # model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
    #     args.model_id,
    #     torch_dtype="auto",
    #     device_map="auto" if args.device == "auto" else None,
    #     trust_remote_code=True,
    # )

    model = Qwen3VLForConditionalGeneration.from_pretrained(
        args.model_id,
        attn_implementation="flash_attention_2",
        dtype=torch.bfloat16,
        device_map="cuda",
    )
    processor = AutoProcessor.from_pretrained(args.model_id, trust_remote_code=True)
    processor.tokenizer.padding_side = "left"


    # é¢å¤–æ‰“å°ä¸‰ç§å¤§å°ï¼Œä¾¿äºè¯Šæ–­
    base_tok_size = tokenizer.vocab_size                  # åŸºç¡€è¯è¡¨å¤§å°
    total_tok_size = len(tokenizer)                       # æ€»è¯è¡¨å¤§å°
    model_embed_size = model.get_input_embeddings().weight.shape[0]  # æ¨¡å‹å½“å‰ embedding å¤§å°
    print(f"[DEBUG] tokenizer.vocab_size(base) = {base_tok_size}")
    print(f"[DEBUG] len(tokenizer)(total)     = {total_tok_size}")
    print(f"[DEBUG] model.embed_size(before)  = {model_embed_size}")
    print(f"[DEBUG] added_in_tokenizer        = {total_tok_size - base_tok_size}")

    mapping, added, action_token_start_idx, action_token_end_idx = add_new_tokens(
        model=model,
        tokenizer=tokenizer,
        new_tokens=tokens,
        init_strategy=args.init_strategy,
        as_special=args.as_special,
    )
    new_model_embed_size = model.get_input_embeddings().weight.shape[0]

    save_bundle(model, tokenizer, mapping, args.save_dir, processor_src=args.model_id, padding_side=args.padding_side)

    # é‡æ–°éªŒè¯
    reload_and_check(args.save_dir, tokens)

    print(f"[INFO] æœ¬æ¬¡æ–°å¢åˆ° tokenizer çš„æ•°é‡: {added}")
    # print(f"[INFO] Token æ˜ å°„: {mapping}")
    print(f"[INFO] Action token idx èŒƒå›´: [{action_token_start_idx}, {action_token_end_idx}]")
    print(f"[DEBUG] model.embed_size(after)   = {new_model_embed_size}")



def start_debugpy_once():
    """start debugpy once"""
    import debugpy
    if getattr(start_debugpy_once, "_started", False):
        return
    debugpy.listen(("0.0.0.0", 10092))
    print("ğŸ” Waiting for VSCode attach on 0.0.0.0:10092 ...")
    debugpy.wait_for_client()
    start_debugpy_once._started = True

if __name__ == "__main__":
    start_debugpy_once()
    main()
