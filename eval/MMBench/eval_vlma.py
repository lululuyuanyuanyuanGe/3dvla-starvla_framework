from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor
from qwen_vl_utils import process_vision_info
from llavavla.model.framework.qwenpi import QwenQFormerDiT
import os, torch


import re
import json

def extract_json_from_string(input_string):
    """
    ä»å­—ç¬¦ä¸²ä¸­æå–æœ‰æ•ˆçš„ JSON éƒ¨åˆ†å¹¶è½¬æ¢ä¸ºå­—å…¸ã€‚
    
    Args:
        input_string (str): åŒ…å«å¤šä½™å­—ç¬¦çš„å­—ç¬¦ä¸²ã€‚
    
    Returns:
        dict: æå–å¹¶è§£æåçš„å­—å…¸ã€‚
    """
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå– JSON éƒ¨åˆ†
    json_match = re.search(r"{.*}", input_string, re.DOTALL)
    if json_match:
        json_str = json_match.group(0)
        try:
            # è½¬æ¢ä¸ºå­—å…¸
            return json.loads(json_str)
        except json.JSONDecodeError as e:
            print(f"JSON è§£ç å¤±è´¥: {e}")
            return None
    else:
        print("æœªæ‰¾åˆ°æœ‰æ•ˆçš„ JSON éƒ¨åˆ†")
        return None

from llavavla.training.trainer_utils.metrics import TrainerUtils
from torchvision.ops import box_iou
import numpy as np

def evaluate_predictions(predicted_solutions, solutions, normalized_actions, actions):
    """
    è¯„ä»·é¢„æµ‹ç»“æœï¼ŒåŒ…æ‹¬ IoU å’ŒåŠ¨ä½œè·ç¦»ã€‚
    
    Args:
        predicted_solutions: List[json]ï¼Œé¢„æµ‹çš„è§£å†³æ–¹æ¡ˆï¼ˆJSONæ ¼å¼ï¼‰ã€‚
        solutions: List[str]ï¼ŒçœŸå®çš„è§£å†³æ–¹æ¡ˆï¼ˆJSONæ ¼å¼å­—ç¬¦ä¸²ï¼‰ã€‚
        normalized_actions: np.ndarrayï¼Œé¢„æµ‹çš„åŠ¨ä½œã€‚
        actions: np.ndarrayï¼ŒçœŸå®çš„åŠ¨ä½œã€‚

    Returns:
        dict: åŒ…å« IoU å’ŒåŠ¨ä½œè·ç¦»çš„è¯„ä»·ç»“æœã€‚
    """
    iou_scores = []
    action_distances = []
    for pred_solution, gt, pre_action, gt_action in zip(predicted_solutions, solutions, normalized_actions, actions):
        # pred_dict = eval(pred_solution)
        try:
            pred_dict = pred_solution
            gt_dict = eval(gt)

            # æå– bbox
            pred_pick_bbox = torch.tensor(pred_dict["pick"]["bbox_2d"], dtype=torch.float32).unsqueeze(0)
            gt_pick_bbox = torch.tensor(gt_dict["pick"]["bbox_2d"], dtype=torch.float32).unsqueeze(0)
            pred_place_bbox = torch.tensor(pred_dict["place"]["bbox_2d"], dtype=torch.float32).unsqueeze(0)
            gt_place_bbox = torch.tensor(gt_dict["place"]["bbox_2d"], dtype=torch.float32).unsqueeze(0)

            # è®¡ç®— IoU
            pick_iou = box_iou(pred_pick_bbox, gt_pick_bbox).item()
            place_iou = box_iou(pred_place_bbox, gt_place_bbox).item()

            # è®¡ç®—åŠ¨ä½œè·ç¦»
            actions = np.array(pre_action)  # ç¡®ä¿ actions æ˜¯ numpy æ•°ç»„
            num_pots = np.prod(pre_action.shape)  # B*len*dim
            action_distance = TrainerUtils.euclidean_distance(pre_action, gt_action)
            average_action_distance = action_distance / num_pots

            # add results
            action_distances.append(average_action_distance)
            iou_scores.append({"pick_iou": pick_iou, "place_iou": place_iou})
        except:
            print(f"Error processing prediction: {pred_solution} or ground truth: {gt}")
            iou_scores.append({"pick_iou": 0.0, "place_iou": 0.0})
            action_distances.append(0.0)

    return {
        "iou_scores": iou_scores,
        "action_distances": action_distances
    }

import debugpy, torch
debugpy.listen(("0.0.0.0", 10092))
print("ğŸ” Rank 0 waiting for debugger attach on port 10092...")
debugpy.wait_for_client()

saved_model_path = "/mnt/petrelfs/yejinhui/Projects/llavavla/results/Checkpoints/0712_vla_v4_vlma/checkpoints/steps_14000_pytorch_model.pt"

qwenpi:QwenQFormerDiT = QwenQFormerDiT.from_pretrained( # a lot of Missing key(s) in state_dict:
          saved_model_path,                       # choose from ['CogACT/CogACT-Small', 'CogACT/CogACT-Base', 'CogACT/CogACT-Large'] or the local path
        )


# default: Load the model on the available device(s)

processor = qwenpi.qwen_vl_interface.processor


cfg = qwenpi.config

from llavavla.dataloader import build_dataloader
from llavavla.training.trainer_utils.metrics import TrainerUtils
import numpy as np
from torch.utils.data import DataLoader

vla_train_dataloader = build_dataloader( # è¿™ä¸ªå†™åœ¨dataload.py å†…éƒ¨
    cfg=cfg)

# æ–¹æ³•2: ä½¿ç”¨è¿­ä»£å™¨
dataset_iter = iter(vla_train_dataloader)
count = 0

total_iou_scores = []
total_avg_action_distance = []
while True and count < 20 :
    try:
        batch_samples = next(dataset_iter)
        count += 1
    except StopIteration:
        break

    examples = batch_samples
    score = 0.0 # æƒ³åŠæ³•çœ‹çœ‹è¯æ˜å˜æˆbatch æ¨ç†
    num_samples = len(examples)

    # @Jinhui TBD TODO 
    images = [example["image"] for example in examples]  #  TODO check æ˜¯ä»€ä¹ˆ
    instructions = [example["lang"] for example in examples]  # [B, str]
    actions = [example["action"] for example in examples] # action label
    solutions = [example["solution"] for example in examples]  # [B, str]
    # Predict actions using the model
    predicted_solutions, normalized_actions = qwenpi.predict_action_withCoT( # TODO è¿™é‡Œæœ‰ æ¨¡å‹æ–¹æ³• ä¾èµ–å…³ç³», å¦‚æœä½ è¦ä¿æŒtrainerçš„ç‹¬ç«‹æ€§ï¼Œè¿™é‡Œåº”è¯¥æ€ä¹ˆè®¾è®¡ï¼Ÿ
        images=images,
        instructions=instructions,
        use_ddim=False,
        num_ddim_steps=20)
    
    # æå–å¹¶è½¬æ¢
    parsed_solutions = []
    for solution in predicted_solutions:
        parsed_solution = extract_json_from_string(solution)
        parsed_solutions.append(parsed_solution)


    # æå‰è½¬æ¢ actions ä¸º numpy.ndarray
    actions = np.array(actions)  # å°† actions è½¬æ¢ä¸º numpy.ndarray (B, len, dim)
    # B, Chunk, dim = actions.shape
    num_pots = np.prod(actions.shape) # B*len*dim
    # Compute the metric score
    score = TrainerUtils.euclidean_distance(normalized_actions, actions)
    average_score = score / num_pots

    # è°ƒç”¨è¯„ä»·å‡½æ•°
    evaluation_results = evaluate_predictions(parsed_solutions, solutions, normalized_actions, actions)

    # print avg score from evaluation_results
    iou_scores = evaluation_results["iou_scores"]
    action_distances = evaluation_results["action_distances"]
    total_iou_scores.extend(iou_scores)
    total_avg_action_distance.extend(action_distances)

    avg_pick_iou = np.mean([iou["pick_iou"] for iou in total_iou_scores])
    avg_place_iou = np.mean([iou["place_iou"] for iou in total_iou_scores])
    avg_action_distance = np.mean(total_avg_action_distance)
    print(f"Batch {count}: Average Pick IoU: {avg_pick_iou:.4f}, Average Place IoU: {avg_place_iou:.4f}, Average Action Distance: {avg_action_distance:.4f}")
