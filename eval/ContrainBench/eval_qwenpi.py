import re
import json
import numpy as np
import torch
from torch.utils.data import DataLoader
from torchvision.ops import box_iou
from llavavla.model.framework.qwenpi import QwenQFormerDiT
from llavavla.training.metrics import TrainerUtils
from llavavla.dataloader import build_dataloader
import debugpy

# Debug setup
debugpy.listen(("0.0.0.0", 10092))
print("ğŸ” Rank 0 waiting for debugger attach on port 10092...")
debugpy.wait_for_client()

class QWenPiModelEvaluator: # TODO å®ƒä¸åº”è¯¥æ˜¯æŸä¸ªæ¨¡å‹çš„æµ‹è¯•ï¼Œ ä»–åº”è¯¥æ˜¯æŸä¸ªbench ä¸ºä¸­å¿ƒçš„ç»‘å®šï¼Œ åªæ˜¯è¦æ±‚æ¨¡å‹è¿”å› bbox å’Œ action
    def __init__(self, model_path, config=None):
        """
        åˆå§‹åŒ–VLAæ¨¡å‹è¯„ä¼°å™¨
        
        Args:
            model_path (str): é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„
            config: æ¨¡å‹é…ç½®å¯¹è±¡
        """
        self.model = QwenQFormerDiT.from_pretrained(model_path)
        self.processor = self.model.qwen_vl_interface.processor
        self.config = config # TODO è¿™é‡Œåº”è¯¥é‡‡ç”¨ä¼ æŸ“çš„è¡Œconfig
        
        # æ„å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
        self.vla_dataset, self.collate_fn = build_dataloader(self.config)
        self.dataloader = DataLoader(
            self.vla_dataset,
            batch_size=self.config.datasets.vla_data.per_device_batch_size,
            collate_fn=self.collate_fn
        )
        
        # è¯„ä¼°ç»“æœå­˜å‚¨
        self.total_iou_scores = []
        self.total_action_distances = []

    @staticmethod
    def extract_json_from_string(input_string):
        """
        ä»å­—ç¬¦ä¸²ä¸­æå–æœ‰æ•ˆçš„JSONéƒ¨åˆ†
        
        Args:
            input_string (str): å¯èƒ½åŒ…å«JSONçš„å­—ç¬¦ä¸²
            
        Returns:
            dict: è§£æåçš„JSONå­—å…¸ï¼Œå¤±è´¥è¿”å›None
        """
        json_match = re.search(r"{.*}", input_string, re.DOTALL)
        if json_match:
            try:
                return json.loads(json_match.group(0))
            except json.JSONDecodeError:
                print(f"JSONè§£ç å¤±è´¥: {input_string}")
                return None
        return None

    def evaluate_predictions(self, predicted_solutions, solutions, normalized_actions, actions):
        """
        è¯„ä¼°é¢„æµ‹ç»“æœ
        
        Args:
            predicted_solutions: é¢„æµ‹çš„è§£å†³æ–¹æ¡ˆåˆ—è¡¨
            solutions: çœŸå®è§£å†³æ–¹æ¡ˆåˆ—è¡¨
            normalized_actions: å½’ä¸€åŒ–çš„é¢„æµ‹åŠ¨ä½œ
            actions: çœŸå®åŠ¨ä½œ
            
        Returns:
            dict: åŒ…å«IoUåˆ†æ•°å’ŒåŠ¨ä½œè·ç¦»çš„è¯„ä¼°ç»“æœ
        """
        batch_iou_scores = []
        batch_action_distances = []
        
        for pred_solution, gt_solution, pred_action, gt_action in zip(
            predicted_solutions, solutions, normalized_actions, actions
        ):
            try:
                # è§£æé¢„æµ‹å’ŒçœŸå®è§£å†³æ–¹æ¡ˆ
                pred_dict = pred_solution if isinstance(pred_solution, dict) else self.extract_json_from_string(pred_solution)
                gt_dict = eval(gt_solution) if isinstance(gt_solution, str) else gt_solution
                
                # è®¡ç®—è¾¹ç•Œæ¡†IoU
                pred_pick = torch.tensor(pred_dict["pick"]["bbox_2d"], dtype=torch.float32).unsqueeze(0)
                gt_pick = torch.tensor(gt_dict["pick"]["bbox_2d"], dtype=torch.float32).unsqueeze(0)
                pred_place = torch.tensor(pred_dict["place"]["bbox_2d"], dtype=torch.float32).unsqueeze(0)
                gt_place = torch.tensor(gt_dict["place"]["bbox_2d"], dtype=torch.float32).unsqueeze(0)
                
                pick_iou = box_iou(pred_pick, gt_pick).item()
                place_iou = box_iou(pred_place, gt_place).item()
                
                # è®¡ç®—åŠ¨ä½œè·ç¦»
                action_distance = TrainerUtils.euclidean_distance(pred_action, gt_action)
                num_elements = np.prod(pred_action.shape)
                avg_action_distance = action_distance / num_elements
                
                batch_iou_scores.append({"pick_iou": pick_iou, "place_iou": place_iou})
                batch_action_distances.append(avg_action_distance)
                
            except Exception as e:
                print(f"å¤„ç†é¢„æµ‹æ—¶å‡ºé”™: {e}")
                batch_iou_scores.append({"pick_iou": 0.0, "place_iou": 0.0})
                batch_action_distances.append(0.0)
        
        return {
            "iou_scores": batch_iou_scores,
            "action_distances": batch_action_distances
        }

    def run_evaluation(self, max_batches=20):
        """
        è¿è¡Œè¯„ä¼°å¾ªç¯
        
        Args:
            max_batches (int): æœ€å¤§è¯„ä¼°æ‰¹æ¬¡æ•°é‡
        """
        dataloader_iter = iter(self.dataloader)
        # TODO æ€ä¹ˆæƒ³åŠæ³•åªæµ‹è¯•å¼€å¤´ç»“å°¾ä¹‹ç±»çš„ key frames? è¿™é‡Œçš„dataloaderï¼Œ æ€ä¹ˆè·å–æ•´æ¡è½¨è¿¹ä¸ºå•ä½è¿›è¡Œæµ‹è¯•ï¼Ÿ
        for batch_idx in range(max_batches):
            try:
                batch = next(dataloader_iter)
            except StopIteration:
                break
            
            # å‡†å¤‡è¾“å…¥æ•°æ®
            images = [example["image"] for example in batch]
            instructions = [example["lang"] for example in batch]
            actions = np.array([example["action"] for example in batch])
            solutions = [example["solution"] for example in batch]
            
            # æ¨¡å‹é¢„æµ‹
            predicted_solutions, normalized_actions = self.model.predict_action(
                images=images,
                instructions=instructions,
                use_ddim=False,
                num_ddim_steps=20
            )
            
            # è§£æé¢„æµ‹ç»“æœ
            parsed_solutions = []
            for solution in predicted_solutions:
                parsed = self.extract_json_from_string(solution)
                parsed_solutions.append(parsed if parsed is not None else {"pick": {"bbox_2d": [0,0,0,0]}, "place": {"bbox_2d": [0,0,0,0]}})
            
            # è¯„ä¼°å½“å‰æ‰¹æ¬¡
            eval_results = self.evaluate_predictions(
                parsed_solutions, solutions, normalized_actions, actions
            )
            
            # æ›´æ–°æ€»ä½“ç»“æœ
            self.total_iou_scores.extend(eval_results["iou_scores"])
            self.total_action_distances.extend(eval_results["action_distances"])
            
            # è®¡ç®—å¹¶æ‰“å°å½“å‰ç»Ÿè®¡ä¿¡æ¯
            avg_pick_iou = np.mean([iou["pick_iou"] for iou in self.total_iou_scores])
            avg_place_iou = np.mean([iou["place_iou"] for iou in self.total_iou_scores])
            avg_action_dist = np.mean(self.total_action_distances)
            
            print(
                f"Batch {batch_idx + 1}: "
                f"Pick IoU: {avg_pick_iou:.4f}, "
                f"Place IoU: {avg_place_iou:.4f}, "
                f"Action Distance: {avg_action_dist:.4f}"
            )

        # æœ€ç»ˆè¯„ä¼°ç»“æœ
        final_results = {
            "average_pick_iou": avg_pick_iou,
            "average_place_iou": avg_place_iou,
            "average_action_distance": avg_action_dist,
            "total_samples": len(self.total_iou_scores)
        }
        
        print("\n=== Final Evaluation Results ===")
        print(json.dumps(final_results, indent=2))
        
        return final_results

# ä½¿ç”¨ç¤ºä¾‹
from omegaconf import OmegaConf
if __name__ == "__main__":
    model_path = "/mnt/petrelfs/yejinhui/Projects/llavavla/results/Checkpoints/0712_vla_v4_vlma/checkpoints/steps_14000_pytorch_model.pt"

    # Load YAML config & Convert CLI overrides to dotlist config
    config_yaml = "llavavla/conf/qwenvla_lmdb_genmanip.yaml"
    cfg = OmegaConf.load(config_yaml)
    
    evaluator = QWenPiModelEvaluator(model_path, config=cfg)
    evaluation_results = evaluator.run_evaluation(max_batches=20)